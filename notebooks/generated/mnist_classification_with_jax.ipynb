{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39f81fc3",
   "metadata": {},
   "source": [
    "# Install and Import Required Libraries\n",
    "Install JAX and other required libraries (e.g., numpy, matplotlib). Import necessary modules from JAX and other libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b442546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Libraries\n",
    "# Uncomment the following lines if running in an environment where JAX is not installed\n",
    "# !pip install jax jaxlib numpy matplotlib\n",
    "\n",
    "# Import Required Libraries\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df1aa27",
   "metadata": {},
   "source": [
    "# Load and Preprocess the MNIST Dataset\n",
    "Download the MNIST dataset, normalize the pixel values, and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc0c6f57",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load and Preprocess the MNIST Dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mnist\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load MNIST dataset\u001b[39;00m\n\u001b[32m      5\u001b[39m (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Load and Preprocess the MNIST Dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to the range [0, 1]\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Flatten the images for input into the neural network\n",
    "train_images = train_images.reshape(-1, 28 * 28)\n",
    "test_images = test_images.reshape(-1, 28 * 28)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels = jax.nn.one_hot(train_labels, 10)\n",
    "test_labels = jax.nn.one_hot(test_labels, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e27a140",
   "metadata": {},
   "source": [
    "# Define the Neural Network Model\n",
    "Use JAX to define a simple feedforward neural network for MNIST classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd12e16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network Model\n",
    "def neural_network(params, x):\n",
    "    # Hidden layer with ReLU activation\n",
    "    hidden = jax.nn.relu(jnp.dot(x, params['W1']) + params['b1'])\n",
    "    # Output layer with softmax activation\n",
    "    logits = jnp.dot(hidden, params['W2']) + params['b2']\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9315a86a",
   "metadata": {},
   "source": [
    "# Initialize Model Parameters\n",
    "Randomly initialize the weights and biases of the neural network using JAX's random module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec9127db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model Parameters\n",
    "def initialize_parameters(key, input_dim, hidden_dim, output_dim):\n",
    "    keys = jax.random.split(key, 2)\n",
    "    params = {\n",
    "        'W1': jax.random.normal(keys[0], (input_dim, hidden_dim)),\n",
    "        'b1': jnp.zeros(hidden_dim),\n",
    "        'W2': jax.random.normal(keys[1], (hidden_dim, output_dim)),\n",
    "        'b2': jnp.zeros(output_dim)\n",
    "    }\n",
    "    return params\n",
    "\n",
    "# Initialize parameters\n",
    "key = jax.random.PRNGKey(0)\n",
    "params = initialize_parameters(key, input_dim=28*28, hidden_dim=128, output_dim=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad905e2",
   "metadata": {},
   "source": [
    "# Define the Loss Function\n",
    "Implement the cross-entropy loss function to measure the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b96da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Loss Function\n",
    "def cross_entropy_loss(params, x, y):\n",
    "    logits = neural_network(params, x)\n",
    "    return -jnp.mean(jnp.sum(y * jax.nn.log_softmax(logits), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32e45eb",
   "metadata": {},
   "source": [
    "# Implement the Training Loop\n",
    "Write a training loop to optimize the model parameters using gradient descent or a similar optimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77e315a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m params\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m params = train_model(params, \u001b[43mtrain_images\u001b[49m, train_labels, epochs=\u001b[32m10\u001b[39m, learning_rate=\u001b[32m0.01\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_images' is not defined"
     ]
    }
   ],
   "source": [
    "# Implement the Training Loop\n",
    "from jax import grad\n",
    "\n",
    "# Define the optimizer step\n",
    "def update_parameters(params, grads, learning_rate):\n",
    "    return {key: params[key] - learning_rate * grads[key] for key in params}\n",
    "\n",
    "# Training loop\n",
    "def train_model(params, train_images, train_labels, epochs, learning_rate):\n",
    "    for epoch in range(epochs):\n",
    "        # Compute gradients\n",
    "        grads = grad(cross_entropy_loss)(params, train_images, train_labels)\n",
    "        # Update parameters\n",
    "        params = update_parameters(params, grads, learning_rate)\n",
    "        # Compute loss\n",
    "        loss = cross_entropy_loss(params, train_images, train_labels)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}\")\n",
    "    return params\n",
    "\n",
    "# Train the model\n",
    "params = train_model(params, train_images, train_labels, epochs=10, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3096cf3f",
   "metadata": {},
   "source": [
    "# Evaluate the Model on Test Data\n",
    "Evaluate the trained model on the test dataset and calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f49f874",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m accuracy = evaluate_model(params, \u001b[43mtest_images\u001b[49m, test_labels)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'test_images' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate the Model on Test Data\n",
    "def evaluate_model(params, test_images, test_labels):\n",
    "    logits = neural_network(params, test_images)\n",
    "    predictions = jnp.argmax(logits, axis=1)\n",
    "    accuracy = jnp.mean(predictions == jnp.argmax(test_labels, axis=1))\n",
    "    return accuracy\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = evaluate_model(params, test_images, test_labels)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c2d718",
   "metadata": {},
   "source": [
    "# Install and Import Required Libraries\n",
    "Install JAX and import necessary libraries such as jax, jax.numpy, and optax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c45e42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install JAX\n",
    "# Note: Uncomment the following line if running in an environment where JAX is not installed.\n",
    "# !pip install jax jaxlib optax tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c61986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89321b7",
   "metadata": {},
   "source": [
    "# Load and Preprocess the MNIST Dataset\n",
    "Use TensorFlow Datasets or another library to load the MNIST dataset and preprocess it by normalizing pixel values and splitting into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d5fa862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /home/codespace/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]0 url/s]\u001b[A\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00, 84.60 url/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00, 63.32 url/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00, 49.81 url/s]\n",
      "\u001b[A\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00, 49.81 url/s]\n",
      "Dl Completed...:  50%|█████     | 1/2 [00:00<00:00, 36.44 url/s]\n",
      "Dl Completed...: 100%|██████████| 2/2 [00:00<00:00, 61.57 url/s]\n",
      "\n",
      "Dl Completed...:  50%|█████     | 1/2 [00:00<00:00, 36.44 url/s]\n",
      "Dl Completed...: 100%|██████████| 2/2 [00:00<00:00, 61.57 url/s]\n",
      "Dl Completed...: 100%|██████████| 2/2 [00:00<00:00, 53.13 url/s]\u001b[A\n",
      "Dl Completed...: 100%|██████████| 2/2 [00:00<00:00, 47.85 url/s]\n",
      "Dl Completed...:  67%|██████▋   | 2/3 [00:00<00:00, 42.13 url/s]\n",
      "Dl Completed...: 100%|██████████| 2/2 [00:00<00:00, 47.85 url/s]689.05 MiB/s]\n",
      "Dl Completed...:  67%|██████▋   | 2/3 [00:00<00:00, 42.13 url/s]\n",
      "Dl Completed...: 100%|██████████| 3/3 [00:00<00:00, 54.63 url/s]\n",
      "Dl Completed...: 100%|██████████| 3/3 [00:00<00:00, 54.63 url/s]\n",
      "Dl Completed...: 100%|██████████| 3/3 [00:00<00:00, 48.36 url/s]\n",
      "Dl Completed...: 100%|██████████| 3/3 [00:00<00:00, 48.36 url/s]\n",
      "Dl Completed...: 100%|██████████| 3/3 [00:00<00:00, 45.02 url/s]\n",
      "Dl Completed...: 100%|██████████| 3/3 [00:00<00:00, 45.02 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00, 41.12 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00, 41.12 url/s]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00, 51.53 url/s]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00, 51.53 url/s]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00, 48.13 url/s]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00, 45.70 url/s]\n",
      "\n",
      "\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00, 45.70 url/s]\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\n",
      "Dl Size...: 100%|██████████| 11594722/11594722 [00:00<00:00, 125684971.76 MiB/s]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00, 42.11 url/s]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00, 42.11 url/s]\n",
      "\n"
     ]
    },
    {
     "ename": "ExtractError",
     "evalue": "Error while extracting /home/codespace/tensorflow_datasets/downloads/mnist/cvdf-datasets_mnist_t10k-images-idx3-ubytejUIsewocHHkkWlvPB_6G4z7q_ueSuEWErsJ29aLbxOY.gz to /home/codespace/tensorflow_datasets/downloads/extracted/GZIP.cvdf-datasets_mnist_t10k-images-idx3-ubytejUIsewocHHkkWlvPB_6G4z7q_ueSuEWErsJ29aLbxOY.gz: No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/tensorflow_datasets/core/download/extractor.py:106\u001b[39m, in \u001b[36m_Extractor._extract\u001b[39m\u001b[34m(self, from_path, method, to_path)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrom_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/tensorflow_datasets/core/download/extractor.py:220\u001b[39m, in \u001b[36miter_gzip\u001b[39m\u001b[34m(arch_f)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34miter_gzip\u001b[39m(arch_f):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_open_or_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43march_f\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgzip_\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgzip\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGzipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/contextlib.py:137\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/tensorflow_datasets/core/download/extractor.py:166\u001b[39m, in \u001b[36m_open_or_pass\u001b[39m\u001b[34m(path_or_fobj)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_fobj, epath.PathLikeCls):\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mio\u001b[49m.gfile.GFile(path_or_fobj, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f_obj:\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m f_obj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/etils/epy/lazy_imports_utils.py:118\u001b[39m, in \u001b[36mLazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_module\u001b[49m, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/functools.py:995\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, instance, owner)\u001b[39m\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[32m--> \u001b[39m\u001b[32m995\u001b[39m     val = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    996\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/etils/epy/lazy_imports_utils.py:79\u001b[39m, in \u001b[36mLazyModule._module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     78\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m _LOCK_PER_MODULE[\u001b[38;5;28mself\u001b[39m.module_name]:\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1324\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mExtractError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[32m      9\u001b[39m ds_builder = tfds.builder(\u001b[33m'\u001b[39m\u001b[33mmnist\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mds_builder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m train_ds = tfds.as_numpy(ds_builder.as_dataset(split=\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m, batch_size=-\u001b[32m1\u001b[39m))\n\u001b[32m     12\u001b[39m test_ds = tfds.as_numpy(ds_builder.as_dataset(split=\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m, batch_size=-\u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/tensorflow_datasets/core/logging/__init__.py:176\u001b[39m, in \u001b[36m_FunctionDecorator.__call__\u001b[39m\u001b[34m(self, function, instance, args, kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m metadata = \u001b[38;5;28mself\u001b[39m._start_call()\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    178\u001b[39m   metadata.mark_error()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/tensorflow_datasets/core/dataset_builder.py:763\u001b[39m, in \u001b[36mDatasetBuilder.download_and_prepare\u001b[39m\u001b[34m(self, download_dir, download_config, file_format, permissions)\u001b[39m\n\u001b[32m    761\u001b[39m   \u001b[38;5;28mself\u001b[39m.info.read_from_directory(\u001b[38;5;28mself\u001b[39m.data_dir)\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m763\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    766\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    768\u001b[39m   \u001b[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[32m    769\u001b[39m   \u001b[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[32m    770\u001b[39m   \u001b[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[32m    771\u001b[39m   \u001b[38;5;66;03m# when reading from package data.\u001b[39;00m\n\u001b[32m    772\u001b[39m   \u001b[38;5;28mself\u001b[39m.info.download_size = dl_manager.downloaded_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/tensorflow_datasets/core/dataset_builder.py:1808\u001b[39m, in \u001b[36mGeneratorBasedBuilder._download_and_prepare\u001b[39m\u001b[34m(self, dl_manager, download_config)\u001b[39m\n\u001b[32m   1805\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m download_config.max_examples_per_split == \u001b[32m0\u001b[39m:\n\u001b[32m   1806\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1808\u001b[39m split_infos = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_splits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1810\u001b[39m \u001b[38;5;66;03m# Update the info object with the splits.\u001b[39;00m\n\u001b[32m   1811\u001b[39m split_dict = splits_lib.SplitDict(split_infos)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/tensorflow_datasets/core/dataset_builder.py:1758\u001b[39m, in \u001b[36mGeneratorBasedBuilder._generate_splits\u001b[39m\u001b[34m(self, dl_manager, download_config)\u001b[39m\n\u001b[32m   1756\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1757\u001b[39m   optional_pipeline_kwargs = {}\n\u001b[32m-> \u001b[39m\u001b[32m1758\u001b[39m split_generators = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_split_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=unexpected-keyword-arg\u001b[39;49;00m\n\u001b[32m   1759\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptional_pipeline_kwargs\u001b[49m\n\u001b[32m   1760\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1761\u001b[39m \u001b[38;5;66;03m# TODO(tfds): Could be removed once all datasets are migrated.\u001b[39;00m\n\u001b[32m   1762\u001b[39m \u001b[38;5;66;03m# https://github.com/tensorflow/datasets/issues/2537\u001b[39;00m\n\u001b[32m   1763\u001b[39m \u001b[38;5;66;03m# Legacy mode (eventually convert list[SplitGeneratorLegacy] -> dict)\u001b[39;00m\n\u001b[32m   1764\u001b[39m split_generators = split_builder.normalize_legacy_split_generators(\n\u001b[32m   1765\u001b[39m     split_generators=split_generators,\n\u001b[32m   1766\u001b[39m     generator_fn=\u001b[38;5;28mself\u001b[39m._generate_examples,\n\u001b[32m   1767\u001b[39m     is_beam=\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, BeamBasedBuilder),\n\u001b[32m   1768\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/tensorflow_datasets/image_classification/mnist.py:119\u001b[39m, in \u001b[36mMNIST._split_generators\u001b[39m\u001b[34m(self, dl_manager)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# Download the full MNIST Database\u001b[39;00m\n\u001b[32m    113\u001b[39m filenames = {\n\u001b[32m    114\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtrain_data\u001b[39m\u001b[33m\"\u001b[39m: _MNIST_TRAIN_DATA_FILENAME,\n\u001b[32m    115\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtrain_labels\u001b[39m\u001b[33m\"\u001b[39m: _MNIST_TRAIN_LABELS_FILENAME,\n\u001b[32m    116\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtest_data\u001b[39m\u001b[33m\"\u001b[39m: _MNIST_TEST_DATA_FILENAME,\n\u001b[32m    117\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtest_labels\u001b[39m\u001b[33m\"\u001b[39m: _MNIST_TEST_LABELS_FILENAME,\n\u001b[32m    118\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m mnist_files = \u001b[43mdl_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_extract\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murllib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m.\u001b[49m\u001b[43murljoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mURL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# MNIST provides TRAIN and TEST splits, not a VALIDATION split, so we only\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# write the TRAIN and TEST splits to disk.\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    126\u001b[39m     tfds.core.SplitGenerator(\n\u001b[32m    127\u001b[39m         name=tfds.Split.TRAIN,\n\u001b[32m   (...)\u001b[39m\u001b[32m    141\u001b[39m     ),\n\u001b[32m    142\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/tensorflow_datasets/core/download/download_manager.py:754\u001b[39m, in \u001b[36mDownloadManager.download_and_extract\u001b[39m\u001b[34m(self, url_or_urls)\u001b[39m\n\u001b[32m    752\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._downloader.tqdm():\n\u001b[32m    753\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._extractor.tqdm():\n\u001b[32m--> \u001b[39m\u001b[32m754\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_promise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_extract\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/tensorflow_datasets/core/download/download_manager.py:782\u001b[39m, in \u001b[36m_map_promise\u001b[39m\u001b[34m(map_fn, all_inputs)\u001b[39m\n\u001b[32m    780\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[32m    781\u001b[39m all_promises = tree.map_structure(map_fn, all_inputs)  \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m res = \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_promises\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait promises\u001b[39;00m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/tree/__init__.py:428\u001b[39m, in \u001b[36mmap_structure\u001b[39m\u001b[34m(func, *structures, **kwargs)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[32m1\u001b[39m:]:\n\u001b[32m    426\u001b[39m   assert_same_structure(structures[\u001b[32m0\u001b[39m], other, check_types=check_types)\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[32m0\u001b[39m],\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m                     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(*\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/tensorflow_datasets/core/download/download_manager.py:782\u001b[39m, in \u001b[36m_map_promise.<locals>.<lambda>\u001b[39m\u001b[34m(p)\u001b[39m\n\u001b[32m    780\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[32m    781\u001b[39m all_promises = tree.map_structure(map_fn, all_inputs)  \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m res = tree.map_structure(\u001b[38;5;28;01mlambda\u001b[39;00m p: \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, all_promises)  \u001b[38;5;66;03m# Wait promises\u001b[39;00m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/promise/promise.py:512\u001b[39m, in \u001b[36mPromise.get\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    510\u001b[39m target = \u001b[38;5;28mself\u001b[39m._target()\n\u001b[32m    511\u001b[39m \u001b[38;5;28mself\u001b[39m._wait(timeout \u001b[38;5;129;01mor\u001b[39;00m DEFAULT_TIMEOUT)\n\u001b[32m--> \u001b[39m\u001b[32m512\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_target_settled_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_raise\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/promise/promise.py:516\u001b[39m, in \u001b[36mPromise._target_settled_value\u001b[39m\u001b[34m(self, _raise)\u001b[39m\n\u001b[32m    514\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_target_settled_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, _raise=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    515\u001b[39m     \u001b[38;5;66;03m# type: (bool) -> Any\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_settled_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_raise\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/promise/promise.py:226\u001b[39m, in \u001b[36mPromise._settled_value\u001b[39m\u001b[34m(self, _raise)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _raise:\n\u001b[32m    225\u001b[39m     raise_val = \u001b[38;5;28mself\u001b[39m._fulfillment_handler0\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraise_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_traceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fulfillment_handler0\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/six.py:724\u001b[39m, in \u001b[36mreraise\u001b[39m\u001b[34m(tp, value, tb)\u001b[39m\n\u001b[32m    722\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m value.__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[32m    723\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m value.with_traceback(tb)\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[32m    725\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    726\u001b[39m     value = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/promise/promise.py:844\u001b[39m, in \u001b[36m_process_future_result.<locals>.handle_future_result\u001b[39m\u001b[34m(future)\u001b[39m\n\u001b[32m    841\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhandle_future_result\u001b[39m(future):\n\u001b[32m    842\u001b[39m     \u001b[38;5;66;03m# type: (Any) -> None\u001b[39;00m\n\u001b[32m    843\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m844\u001b[39m         resolve(\u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    846\u001b[39m         tb = exc_info()[\u001b[32m2\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/concurrent/futures/thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/tensorflow_datasets/core/download/extractor.py:128\u001b[39m, in \u001b[36m_Extractor._extract\u001b[39m\u001b[34m(self, from_path, method, to_path)\u001b[39m\n\u001b[32m    122\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m os.name == \u001b[33m'\u001b[39m\u001b[33mnt\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m max_length_dst_path > \u001b[32m250\u001b[39m:\n\u001b[32m    123\u001b[39m     msg += (\n\u001b[32m    124\u001b[39m         \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOn windows, path lengths greater than 260 characters may result\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    125\u001b[39m         \u001b[33m'\u001b[39m\u001b[33m in an error. See the doc to remove the limitation: \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    126\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mhttps://docs.python.org/3/using/windows.html#removing-the-max-path-limitation\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    127\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ExtractError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# `tf.io.gfile.Rename(overwrite=True)` doesn't work for non empty\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# directories, so delete destination first, if it already exists.\u001b[39;00m\n\u001b[32m    132\u001b[39m to_path = epath.Path(to_path)\n",
      "\u001b[31mExtractError\u001b[39m: Error while extracting /home/codespace/tensorflow_datasets/downloads/mnist/cvdf-datasets_mnist_t10k-images-idx3-ubytejUIsewocHHkkWlvPB_6G4z7q_ueSuEWErsJ29aLbxOY.gz to /home/codespace/tensorflow_datasets/downloads/extracted/GZIP.cvdf-datasets_mnist_t10k-images-idx3-ubytejUIsewocHHkkWlvPB_6G4z7q_ueSuEWErsJ29aLbxOY.gz: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Load and Preprocess the MNIST Dataset\n",
    "def preprocess_data(data):\n",
    "    \"\"\"Normalize pixel values and convert labels to one-hot encoding.\"\"\"\n",
    "    images = data['image'] / 255.0\n",
    "    labels = jax.nn.one_hot(data['label'], num_classes=10)\n",
    "    return images, labels\n",
    "\n",
    "# Load dataset\n",
    "ds_builder = tfds.builder('mnist')\n",
    "ds_builder.download_and_prepare()\n",
    "train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))\n",
    "test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))\n",
    "\n",
    "# Preprocess dataset\n",
    "train_images, train_labels = preprocess_data(train_ds)\n",
    "test_images, test_labels = preprocess_data(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f427363e",
   "metadata": {},
   "source": [
    "# Define the Neural Network Model\n",
    "Define a simple feedforward neural network using JAX functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c60018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network Model\n",
    "def neural_network(params, inputs):\n",
    "    \"\"\"Feedforward neural network.\"\"\"\n",
    "    hidden = jax.nn.relu(jnp.dot(inputs, params['w1']) + params['b1'])\n",
    "    logits = jnp.dot(hidden, params['w2']) + params['b2']\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852e5b30",
   "metadata": {},
   "source": [
    "# Initialize Parameters\n",
    "Randomly initialize the weights and biases of the neural network using JAX's random module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5af7ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Parameters\n",
    "def initialize_parameters(key, input_dim, hidden_dim, output_dim):\n",
    "    \"\"\"Initialize weights and biases for the neural network.\"\"\"\n",
    "    keys = jax.random.split(key, 3)\n",
    "    params = {\n",
    "        'w1': jax.random.normal(keys[0], (input_dim, hidden_dim)),\n",
    "        'b1': jnp.zeros(hidden_dim),\n",
    "        'w2': jax.random.normal(keys[1], (hidden_dim, output_dim)),\n",
    "        'b2': jnp.zeros(output_dim)\n",
    "    }\n",
    "    return params\n",
    "\n",
    "# Set dimensions\n",
    "input_dim = 28 * 28\n",
    "hidden_dim = 128\n",
    "output_dim = 10\n",
    "\n",
    "# Initialize parameters\n",
    "key = jax.random.PRNGKey(0)\n",
    "params = initialize_parameters(key, input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25976a3",
   "metadata": {},
   "source": [
    "# Define the Loss Function and Accuracy Metric\n",
    "Implement a cross-entropy loss function and an accuracy metric to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6528a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Loss Function and Accuracy Metric\n",
    "def cross_entropy_loss(params, inputs, targets):\n",
    "    \"\"\"Compute the cross-entropy loss.\"\"\"\n",
    "    logits = neural_network(params, inputs)\n",
    "    return -jnp.mean(jnp.sum(targets * jax.nn.log_softmax(logits), axis=1))\n",
    "\n",
    "def compute_accuracy(params, inputs, targets):\n",
    "    \"\"\"Compute the accuracy of the model.\"\"\"\n",
    "    logits = neural_network(params, inputs)\n",
    "    predictions = jnp.argmax(logits, axis=1)\n",
    "    targets = jnp.argmax(targets, axis=1)\n",
    "    return jnp.mean(predictions == targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d86004",
   "metadata": {},
   "source": [
    "# Implement the Training Loop\n",
    "Write a training loop that performs forward and backward passes, updates parameters using an optimizer, and logs training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87458c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Training Loop\n",
    "def train_model(params, train_images, train_labels, test_images, test_labels, num_epochs, learning_rate):\n",
    "    \"\"\"Train the neural network.\"\"\"\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Compute gradients\n",
    "        loss, grads = jax.value_and_grad(cross_entropy_loss)(params, train_images, train_labels)\n",
    "        \n",
    "        # Update parameters\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        \n",
    "        # Compute accuracy\n",
    "        train_acc = compute_accuracy(params, train_images, train_labels)\n",
    "        test_acc = compute_accuracy(params, test_images, test_labels)\n",
    "        \n",
    "        # Log progress\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}, Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    return params\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "trained_params = train_model(params, train_images, train_labels, test_images, test_labels, num_epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ce7e2e",
   "metadata": {},
   "source": [
    "# Evaluate the Model on Test Data\n",
    "Evaluate the trained model on the test dataset and report the final accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a209e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model on Test Data\n",
    "final_test_accuracy = compute_accuracy(trained_params, test_images, test_labels)\n",
    "print(f\"Final Test Accuracy: {final_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459d29ef",
   "metadata": {},
   "source": [
    "# Install and Import Required Libraries\n",
    "Install JAX and other required libraries (e.g., numpy, matplotlib, tensorflow_datasets). Import all necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443832f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Libraries\n",
    "# Uncomment the following lines if running in an environment where JAX and other libraries are not installed.\n",
    "# !pip install jax jaxlib numpy matplotlib tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d985271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598d8748",
   "metadata": {},
   "source": [
    "# Load and Preprocess the MNIST Dataset\n",
    "Use tensorflow_datasets to load the MNIST dataset. Normalize the images and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b20575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST Dataset\n",
    "ds_builder = tfds.builder(\"mnist\")\n",
    "ds_builder.download_and_prepare()\n",
    "datasets = ds_builder.as_dataset(as_supervised=True)\n",
    "\n",
    "# Normalize and split the dataset\n",
    "def preprocess(image, label):\n",
    "    image = jnp.array(image) / 255.0  # Normalize to [0, 1]\n",
    "    label = jnp.array(label)\n",
    "    return image, label\n",
    "\n",
    "train_dataset = datasets[\"train\"].map(preprocess)\n",
    "test_dataset = datasets[\"test\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a2fc3a",
   "metadata": {},
   "source": [
    "# Define the Neural Network Model\n",
    "Use JAX to define a simple feedforward neural network with layers for MNIST classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c59fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Neural Network Model\n",
    "def relu(x):\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = jnp.exp(x - jnp.max(x))\n",
    "    return exp_x / exp_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "def neural_network(params, x):\n",
    "    hidden = relu(jnp.dot(x, params[\"W1\"]) + params[\"b1\"])\n",
    "    logits = jnp.dot(hidden, params[\"W2\"]) + params[\"b2\"]\n",
    "    return softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c649e99",
   "metadata": {},
   "source": [
    "# Initialize Parameters\n",
    "Randomly initialize the weights and biases of the neural network using JAX's random module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f40f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Parameters\n",
    "def initialize_params(key):\n",
    "    input_size = 28 * 28  # MNIST images are 28x28\n",
    "    hidden_size = 128\n",
    "    output_size = 10  # 10 classes for digits 0-9\n",
    "\n",
    "    keys = jax.random.split(key, 3)\n",
    "    params = {\n",
    "        \"W1\": jax.random.normal(keys[0], (input_size, hidden_size)),\n",
    "        \"b1\": jnp.zeros(hidden_size),\n",
    "        \"W2\": jax.random.normal(keys[1], (hidden_size, output_size)),\n",
    "        \"b2\": jnp.zeros(output_size),\n",
    "    }\n",
    "    return params\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "params = initialize_params(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f32626",
   "metadata": {},
   "source": [
    "# Define the Loss Function\n",
    "Implement a cross-entropy loss function to compute the error between predictions and true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dcbfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss Function\n",
    "def cross_entropy_loss(params, x, y):\n",
    "    predictions = neural_network(params, x)\n",
    "    one_hot_y = jax.nn.one_hot(y, num_classes=10)\n",
    "    return -jnp.mean(jnp.sum(one_hot_y * jnp.log(predictions), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e29668",
   "metadata": {},
   "source": [
    "# Define the Training Loop\n",
    "Write a training loop that uses JAX's grad function to compute gradients and update the model parameters using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc991cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Loop\n",
    "@jax.jit\n",
    "def update_params(params, x, y, learning_rate):\n",
    "    grads = jax.grad(cross_entropy_loss)(params, x, y)\n",
    "    return jax.tree_util.tree_map(lambda p, g: p - learning_rate * g, params, grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151514d7",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "Run the training loop for a specified number of epochs and monitor the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7231db42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_dataset.batch(32):\n",
    "        images, labels = batch\n",
    "        images = images.reshape(-1, 28 * 28)  # Flatten images\n",
    "        params = update_params(params, images, labels, learning_rate)\n",
    "    print(f\"Epoch {epoch + 1} completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a4d888",
   "metadata": {},
   "source": [
    "# Evaluate the Model\n",
    "Evaluate the trained model on the test dataset and compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69021183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "def compute_accuracy(params, dataset):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in dataset.batch(32):\n",
    "        images, labels = batch\n",
    "        images = images.reshape(-1, 28 * 28)  # Flatten images\n",
    "        predictions = neural_network(params, images)\n",
    "        predicted_labels = jnp.argmax(predictions, axis=-1)\n",
    "        correct += jnp.sum(predicted_labels == labels)\n",
    "        total += len(labels)\n",
    "    return correct / total\n",
    "\n",
    "accuracy = compute_accuracy(params, test_dataset)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c04f8d",
   "metadata": {},
   "source": [
    "# Install and Import Required Libraries\n",
    "Install JAX and other required libraries (e.g., numpy, matplotlib, tensorflow_datasets). Import all necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13475470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Libraries\n",
    "# Uncomment the following lines if running in an environment where JAX and other libraries are not installed.\n",
    "# !pip install jax jaxlib numpy matplotlib tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8f643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d93f78",
   "metadata": {},
   "source": [
    "# Load and Preprocess the MNIST Dataset\n",
    "Use tensorflow_datasets to load the MNIST dataset. Normalize the images and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aafec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST Dataset\n",
    "ds_builder = tfds.builder(\"mnist\")\n",
    "ds_builder.download_and_prepare()\n",
    "datasets = ds_builder.as_dataset(as_supervised=True)\n",
    "\n",
    "# Normalize and split the data\n",
    "def preprocess(image, label):\n",
    "    image = jnp.array(image) / 255.0  # Normalize to [0, 1]\n",
    "    label = jnp.array(label)\n",
    "    return image, label\n",
    "\n",
    "train_dataset = datasets[\"train\"].map(preprocess)\n",
    "test_dataset = datasets[\"test\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048f8492",
   "metadata": {},
   "source": [
    "# Define the Neural Network Model\n",
    "Use JAX to define a simple feedforward neural network with layers for MNIST classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27910250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network Model\n",
    "def relu(x):\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = jnp.exp(x - jnp.max(x))\n",
    "    return exp_x / jnp.sum(exp_x)\n",
    "\n",
    "def neural_network(params, x):\n",
    "    hidden = relu(jnp.dot(x, params[\"W1\"]) + params[\"b1\"])\n",
    "    logits = jnp.dot(hidden, params[\"W2\"]) + params[\"b2\"]\n",
    "    return softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4295e42c",
   "metadata": {},
   "source": [
    "# Initialize Parameters\n",
    "Randomly initialize the weights and biases of the neural network using JAX's random module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd6c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Parameters\n",
    "def initialize_parameters(rng, input_size, hidden_size, output_size):\n",
    "    params = {\n",
    "        \"W1\": jax.random.normal(rng, (input_size, hidden_size)),\n",
    "        \"b1\": jnp.zeros(hidden_size),\n",
    "        \"W2\": jax.random.normal(rng, (hidden_size, output_size)),\n",
    "        \"b2\": jnp.zeros(output_size),\n",
    "    }\n",
    "    return params\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "params = initialize_parameters(rng, input_size=784, hidden_size=128, output_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6ee2c9",
   "metadata": {},
   "source": [
    "# Define the Loss Function and Accuracy Metric\n",
    "Implement the cross-entropy loss function and accuracy metric using JAX functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0342a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss Function and Accuracy Metric\n",
    "def cross_entropy_loss(params, x, y):\n",
    "    predictions = neural_network(params, x)\n",
    "    return -jnp.mean(jnp.sum(y * jnp.log(predictions), axis=1))\n",
    "\n",
    "def accuracy(params, x, y):\n",
    "    predictions = neural_network(params, x)\n",
    "    predicted_labels = jnp.argmax(predictions, axis=1)\n",
    "    true_labels = jnp.argmax(y, axis=1)\n",
    "    return jnp.mean(predicted_labels == true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473da2a0",
   "metadata": {},
   "source": [
    "# Implement the Training Loop\n",
    "Write a training loop to update the model parameters using gradient descent and JAX's grad function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d1357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "@jax.jit\n",
    "def update_parameters(params, x, y, learning_rate):\n",
    "    grads = jax.grad(cross_entropy_loss)(params, x, y)\n",
    "    updated_params = {key: params[key] - learning_rate * grads[key] for key in params}\n",
    "    return updated_params\n",
    "\n",
    "# Example Training Loop\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_dataset.batch(32):\n",
    "        images, labels = batch\n",
    "        params = update_parameters(params, images, labels, learning_rate)\n",
    "    print(f\"Epoch {epoch + 1} completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e230d6",
   "metadata": {},
   "source": [
    "# Evaluate the Model on Test Data\n",
    "Evaluate the trained model on the test dataset and calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018c2151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "test_images, test_labels = next(iter(test_dataset.batch(10000)))\n",
    "test_acc = accuracy(params, test_images, test_labels)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996023be",
   "metadata": {},
   "source": [
    "# Install and Import Required Libraries\n",
    "Install JAX and import necessary libraries such as jax, jax.numpy, and optax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafd1541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install JAX\n",
    "# Note: Uncomment the following line to install JAX in your environment\n",
    "# !pip install jax jaxlib optax\n",
    "\n",
    "# Import Required Libraries\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2268b34",
   "metadata": {},
   "source": [
    "# Load and Preprocess the MNIST Dataset\n",
    "Load the MNIST dataset, normalize the pixel values, and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2008150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Preprocess the MNIST Dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to the range [0, 1]\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Flatten images\n",
    "train_images = train_images.reshape(-1, 28 * 28)\n",
    "test_images = test_images.reshape(-1, 28 * 28)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels = jax.nn.one_hot(train_labels, num_classes=10)\n",
    "test_labels = jax.nn.one_hot(test_labels, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae63d6b",
   "metadata": {},
   "source": [
    "# Define the Neural Network Model\n",
    "Use JAX to define a simple feedforward neural network for MNIST classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6622f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network Model\n",
    "def neural_network(params, x):\n",
    "    for w, b in params[:-1]:\n",
    "        x = jnp.dot(x, w) + b\n",
    "        x = jax.nn.relu(x)\n",
    "    final_w, final_b = params[-1]\n",
    "    x = jnp.dot(x, final_w) + final_b\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4e2bfa",
   "metadata": {},
   "source": [
    "# Initialize Parameters\n",
    "Initialize the model parameters using JAX's random number generation utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0745e086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Parameters\n",
    "def initialize_parameters(layer_sizes, key):\n",
    "    params = []\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        key, subkey = random.split(key)\n",
    "        w = random.normal(subkey, (layer_sizes[i], layer_sizes[i + 1])) * jnp.sqrt(2.0 / layer_sizes[i])\n",
    "        b = jnp.zeros((layer_sizes[i + 1],))\n",
    "        params.append((w, b))\n",
    "    return params\n",
    "\n",
    "# Define layer sizes\n",
    "layer_sizes = [28 * 28, 128, 64, 10]\n",
    "key = random.PRNGKey(0)\n",
    "params = initialize_parameters(layer_sizes, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da942c1b",
   "metadata": {},
   "source": [
    "# Define the Loss Function\n",
    "Implement a cross-entropy loss function to compute the difference between predictions and true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6980328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Loss Function\n",
    "def cross_entropy_loss(params, x, y):\n",
    "    logits = neural_network(params, x)\n",
    "    return -jnp.mean(jnp.sum(y * jax.nn.log_softmax(logits), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764a8f48",
   "metadata": {},
   "source": [
    "# Define the Training Loop\n",
    "Write a training loop that updates model parameters using gradient descent or an optimizer from Optax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029e928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Training Loop\n",
    "@jax.jit\n",
    "def update(params, x, y, opt_state, optimizer):\n",
    "    grads = jax.grad(cross_entropy_loss)(params, x, y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb71c690",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "Run the training loop on the training dataset and monitor the loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360c5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(train_images), batch_size):\n",
    "        x_batch = train_images[i:i + batch_size]\n",
    "        y_batch = train_labels[i:i + batch_size]\n",
    "        params, opt_state = update(params, x_batch, y_batch, opt_state, optimizer)\n",
    "    # Compute loss for the epoch\n",
    "    epoch_loss = cross_entropy_loss(params, train_images, train_labels)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269fc0be",
   "metadata": {},
   "source": [
    "# Evaluate the Model\n",
    "Evaluate the trained model on the test dataset and compute the final accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9c67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "def accuracy(params, x, y):\n",
    "    predictions = jnp.argmax(neural_network(params, x), axis=1)\n",
    "    targets = jnp.argmax(y, axis=1)\n",
    "    return jnp.mean(predictions == targets)\n",
    "\n",
    "test_accuracy = accuracy(params, test_images, test_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c832f90a",
   "metadata": {},
   "source": [
    "# Install and Import Required Libraries\n",
    "Install JAX and other required libraries (e.g., numpy, matplotlib, tensorflow_datasets). Import necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c713c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Libraries\n",
    "!pip install jax jaxlib numpy matplotlib tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5d3811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723407ef",
   "metadata": {},
   "source": [
    "# Load and Preprocess the MNIST Dataset\n",
    "Use tensorflow_datasets to load the MNIST dataset. Normalize the images and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254fd3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST Dataset\n",
    "def load_and_preprocess_data():\n",
    "    ds_train, ds_test = tfds.load('mnist', split=['train', 'test'], as_supervised=True)\n",
    "    \n",
    "    def normalize_img(image, label):\n",
    "        image = jnp.array(image) / 255.0  # Normalize to [0, 1]\n",
    "        label = jnp.array(label)\n",
    "        return image, label\n",
    "    \n",
    "    ds_train = ds_train.map(normalize_img)\n",
    "    ds_test = ds_test.map(normalize_img)\n",
    "    \n",
    "    return ds_train, ds_test\n",
    "\n",
    "ds_train, ds_test = load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90dd09b",
   "metadata": {},
   "source": [
    "# Define the Neural Network Model\n",
    "Use JAX to define a simple feedforward neural network with layers for MNIST classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b5d9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Neural Network Model\n",
    "def relu(x):\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = jnp.exp(x - jnp.max(x))\n",
    "    return exp_x / exp_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "def neural_net(params, x):\n",
    "    hidden = relu(jnp.dot(x, params['W1']) + params['b1'])\n",
    "    logits = jnp.dot(hidden, params['W2']) + params['b2']\n",
    "    return softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f4fc62",
   "metadata": {},
   "source": [
    "# Initialize Parameters\n",
    "Randomly initialize the weights and biases of the neural network using JAX's random module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df8b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Parameters\n",
    "def initialize_parameters(rng, input_dim, hidden_dim, output_dim):\n",
    "    params = {\n",
    "        'W1': jax.random.normal(rng, (input_dim, hidden_dim)),\n",
    "        'b1': jnp.zeros(hidden_dim),\n",
    "        'W2': jax.random.normal(rng, (hidden_dim, output_dim)),\n",
    "        'b2': jnp.zeros(output_dim)\n",
    "    }\n",
    "    return params\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "params = initialize_parameters(rng, input_dim=784, hidden_dim=128, output_dim=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250b6cf7",
   "metadata": {},
   "source": [
    "# Define the Loss Function\n",
    "Implement a cross-entropy loss function to compute the error between predictions and true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f08674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss Function\n",
    "def cross_entropy_loss(params, x, y):\n",
    "    predictions = neural_net(params, x)\n",
    "    one_hot_y = jax.nn.one_hot(y, num_classes=10)\n",
    "    return -jnp.mean(jnp.sum(one_hot_y * jnp.log(predictions), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd6423a",
   "metadata": {},
   "source": [
    "# Define the Training Loop\n",
    "Write a training loop that uses JAX's grad function to compute gradients and update parameters using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627f2aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Loop\n",
    "@jax.jit\n",
    "def update_parameters(params, x, y, learning_rate):\n",
    "    grads = jax.grad(cross_entropy_loss)(params, x, y)\n",
    "    updated_params = jax.tree_util.tree_map(lambda p, g: p - learning_rate * g, params, grads)\n",
    "    return updated_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2ac0d5",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "Run the training loop for a specified number of epochs and track the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d2a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "def train_model(params, ds_train, epochs, learning_rate):\n",
    "    for epoch in range(epochs):\n",
    "        for batch in ds_train.batch(128):\n",
    "            images, labels = batch\n",
    "            images = images.reshape(-1, 784)  # Flatten images\n",
    "            params = update_parameters(params, images, labels, learning_rate)\n",
    "        print(f\"Epoch {epoch + 1} completed.\")\n",
    "    return params\n",
    "\n",
    "params = train_model(params, ds_train, epochs=10, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028efdf7",
   "metadata": {},
   "source": [
    "# Evaluate the Model\n",
    "Evaluate the trained model on the test dataset and compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f93541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "def evaluate_model(params, ds_test):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in ds_test.batch(128):\n",
    "        images, labels = batch\n",
    "        images = images.reshape(-1, 784)  # Flatten images\n",
    "        predictions = neural_net(params, images)\n",
    "        predicted_labels = jnp.argmax(predictions, axis=-1)\n",
    "        correct += jnp.sum(predicted_labels == labels)\n",
    "        total += len(labels)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "evaluate_model(params, ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc878a24",
   "metadata": {},
   "source": [
    "# Install and Import Required Libraries\n",
    "Install JAX and import necessary libraries such as jax, jax.numpy, and optax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c55ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install JAX\n",
    "# Note: Uncomment the following line to install JAX if not already installed.\n",
    "# !pip install jax jaxlib optax tensorflow-datasets\n",
    "\n",
    "# Import Required Libraries\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3edeca6",
   "metadata": {},
   "source": [
    "# Load and Preprocess the MNIST Dataset\n",
    "Use TensorFlow Datasets to load the MNIST dataset and preprocess it by normalizing pixel values and converting labels to one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec233d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST Dataset\n",
    "def load_and_preprocess_data():\n",
    "    ds_train, ds_test = tfds.load('mnist', split=['train', 'test'], as_supervised=True)\n",
    "    \n",
    "    def preprocess(image, label):\n",
    "        image = jnp.array(image) / 255.0  # Normalize pixel values\n",
    "        label = jax.nn.one_hot(label, 10)  # Convert labels to one-hot encoding\n",
    "        return image, label\n",
    "    \n",
    "    ds_train = ds_train.map(preprocess)\n",
    "    ds_test = ds_test.map(preprocess)\n",
    "    return ds_train, ds_test\n",
    "\n",
    "train_data, test_data = load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ba20d8",
   "metadata": {},
   "source": [
    "# Define the Neural Network Model\n",
    "Define a simple feedforward neural network using JAX functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3814a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Neural Network Model\n",
    "def neural_network(params, x):\n",
    "    for w, b in params[:-1]:\n",
    "        x = jnp.dot(x, w) + b\n",
    "        x = jax.nn.relu(x)\n",
    "    w, b = params[-1]\n",
    "    x = jnp.dot(x, w) + b\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfe21ee",
   "metadata": {},
   "source": [
    "# Initialize Parameters\n",
    "Initialize the weights and biases of the neural network using JAX's random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64fccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Parameters\n",
    "def initialize_parameters(layer_sizes, key):\n",
    "    params = []\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        w = jax.random.normal(subkey, (layer_sizes[i], layer_sizes[i + 1]))\n",
    "        b = jnp.zeros((layer_sizes[i + 1],))\n",
    "        params.append((w, b))\n",
    "    return params\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "layer_sizes = [784, 128, 64, 10]  # Input layer, two hidden layers, output layer\n",
    "params = initialize_parameters(layer_sizes, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cb1513",
   "metadata": {},
   "source": [
    "# Define the Loss Function\n",
    "Implement the cross-entropy loss function to compute the difference between predictions and true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2f497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss Function\n",
    "def cross_entropy_loss(params, x, y):\n",
    "    logits = neural_network(params, x)\n",
    "    return -jnp.mean(jnp.sum(y * jax.nn.log_softmax(logits), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c088ba",
   "metadata": {},
   "source": [
    "# Implement the Training Loop\n",
    "Write a training loop that updates the model parameters using gradient descent and evaluates the loss on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13bba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_model(params, train_data, learning_rate, epochs):\n",
    "    optimizer = optax.sgd(learning_rate)\n",
    "    opt_state = optimizer.init(params)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for x, y in train_data:\n",
    "            loss, grads = jax.value_and_grad(cross_entropy_loss)(params, x, y)\n",
    "            updates, opt_state = optimizer.update(grads, opt_state)\n",
    "            params = optax.apply_updates(params, updates)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss}\")\n",
    "    return params\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 5\n",
    "trained_params = train_model(params, train_data, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fd1f51",
   "metadata": {},
   "source": [
    "# Evaluate the Model on Test Data\n",
    "Evaluate the trained model on the test dataset and compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f8060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "def evaluate_model(params, test_data):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in test_data:\n",
    "        logits = neural_network(params, x)\n",
    "        predictions = jnp.argmax(logits, axis=-1)\n",
    "        labels = jnp.argmax(y, axis=-1)\n",
    "        correct += jnp.sum(predictions == labels)\n",
    "        total += len(labels)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "evaluate_model(trained_params, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb73b17f",
   "metadata": {},
   "source": [
    "# Install and Import Required Libraries\n",
    "Install JAX and other required libraries (e.g., numpy, matplotlib, tensorflow_datasets). Import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e5f921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Libraries\n",
    "# Uncomment the following lines if running in an environment where JAX and other libraries are not installed.\n",
    "# !pip install jax jaxlib numpy matplotlib tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823afa9e",
   "metadata": {},
   "source": [
    "# Load and Preprocess the MNIST Dataset\n",
    "Use tensorflow_datasets to load the MNIST dataset. Normalize the images and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0df61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST Dataset\n",
    "ds_builder = tfds.builder(\"mnist\")\n",
    "ds_builder.download_and_prepare()\n",
    "datasets = ds_builder.as_dataset(as_supervised=True)\n",
    "\n",
    "# Normalize and split the data\n",
    "def preprocess(image, label):\n",
    "    image = jnp.array(image) / 255.0  # Normalize to [0, 1]\n",
    "    label = jnp.array(label)\n",
    "    return image, label\n",
    "\n",
    "train_ds = datasets[\"train\"].map(preprocess)\n",
    "test_ds = datasets[\"test\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdc14b9",
   "metadata": {},
   "source": [
    "# Define the Neural Network Model\n",
    "Use JAX to define a simple feedforward neural network with layers for MNIST classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fe0c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Neural Network Model\n",
    "def relu(x):\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = jnp.exp(x - jnp.max(x))\n",
    "    return exp_x / jnp.sum(exp_x)\n",
    "\n",
    "def neural_net(params, x):\n",
    "    hidden = relu(jnp.dot(x, params[\"W1\"]) + params[\"b1\"])\n",
    "    logits = jnp.dot(hidden, params[\"W2\"]) + params[\"b2\"]\n",
    "    return softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37100b24",
   "metadata": {},
   "source": [
    "# Initialize Model Parameters\n",
    "Randomly initialize the weights and biases of the neural network using JAX's random module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd354d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Parameters\n",
    "def initialize_params(key, input_dim, hidden_dim, output_dim):\n",
    "    key1, key2 = jax.random.split(key)\n",
    "    params = {\n",
    "        \"W1\": jax.random.normal(key1, (input_dim, hidden_dim)),\n",
    "        \"b1\": jnp.zeros(hidden_dim),\n",
    "        \"W2\": jax.random.normal(key2, (hidden_dim, output_dim)),\n",
    "        \"b2\": jnp.zeros(output_dim),\n",
    "    }\n",
    "    return params\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "params = initialize_params(key, input_dim=28*28, hidden_dim=128, output_dim=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234bb6a1",
   "metadata": {},
   "source": [
    "# Define the Loss Function\n",
    "Implement a cross-entropy loss function to compute the model's loss during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a799f882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss Function\n",
    "def cross_entropy_loss(params, x, y):\n",
    "    predictions = neural_net(params, x)\n",
    "    one_hot_y = jax.nn.one_hot(y, num_classes=10)\n",
    "    return -jnp.mean(jnp.sum(one_hot_y * jnp.log(predictions), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4c9507",
   "metadata": {},
   "source": [
    "# Implement the Training Loop\n",
    "Write a training loop that uses JAX's grad function to compute gradients and update the model parameters using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93535b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "@jax.jit\n",
    "def update_params(params, x, y, learning_rate):\n",
    "    grads = jax.grad(cross_entropy_loss)(params, x, y)\n",
    "    return jax.tree_util.tree_map(lambda p, g: p - learning_rate * g, params, grads)\n",
    "\n",
    "# Example training loop (pseudo-code, adjust for batching)\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch_x, batch_y in train_ds:\n",
    "#         params = update_params(params, batch_x, batch_y, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b204be72",
   "metadata": {},
   "source": [
    "# Evaluate the Model on Test Data\n",
    "Evaluate the trained model on the test dataset and compute the accuracy. Visualize some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafd616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "def evaluate_model(params, test_ds):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in test_ds:\n",
    "        predictions = neural_net(params, x)\n",
    "        predicted_labels = jnp.argmax(predictions, axis=1)\n",
    "        correct += jnp.sum(predicted_labels == y)\n",
    "        total += len(y)\n",
    "    return correct / total\n",
    "\n",
    "# Example evaluation (pseudo-code, adjust for batching)\n",
    "# accuracy = evaluate_model(params, test_ds)\n",
    "# print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68739ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Predictions\n",
    "def visualize_predictions(params, test_ds, num_images=5):\n",
    "    for i, (x, y) in enumerate(test_ds.take(num_images)):\n",
    "        predictions = neural_net(params, x)\n",
    "        predicted_label = jnp.argmax(predictions)\n",
    "        plt.imshow(x.reshape(28, 28), cmap=\"gray\")\n",
    "        plt.title(f\"True: {y}, Predicted: {predicted_label}\")\n",
    "        plt.show()\n",
    "\n",
    "# Example visualization (pseudo-code)\n",
    "# visualize_predictions(params, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a97bbc",
   "metadata": {},
   "source": [
    "# Install and Import Required Libraries\n",
    "Install JAX and import necessary libraries such as jax, jax.numpy, and optax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da559f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install JAX\n",
    "# Note: Uncomment the following line to install JAX if not already installed\n",
    "# !pip install jax jaxlib optax tensorflow-datasets\n",
    "\n",
    "# Import Required Libraries\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cb3a82",
   "metadata": {},
   "source": [
    "# Load and Preprocess the MNIST Dataset\n",
    "Load the MNIST dataset using TensorFlow Datasets or another library, normalize the images, and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4773ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST Dataset\n",
    "def load_and_preprocess_data():\n",
    "    dataset = tfds.load('mnist', as_supervised=True)\n",
    "    train_data, test_data = dataset['train'], dataset['test']\n",
    "\n",
    "    def normalize_img(image, label):\n",
    "        image = jnp.array(image) / 255.0  # Normalize to [0, 1]\n",
    "        return image, label\n",
    "\n",
    "    train_data = train_data.map(normalize_img)\n",
    "    test_data = test_data.map(normalize_img)\n",
    "    return train_data, test_data\n",
    "\n",
    "train_data, test_data = load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c66d04",
   "metadata": {},
   "source": [
    "# Define the Neural Network Model\n",
    "Use JAX to define a simple feedforward neural network for MNIST classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e24563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Neural Network Model\n",
    "def neural_network(params, x):\n",
    "    for w, b in params[:-1]:\n",
    "        x = jnp.dot(x, w) + b\n",
    "        x = jax.nn.relu(x)\n",
    "    w, b = params[-1]\n",
    "    x = jnp.dot(x, w) + b\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdbaaa8",
   "metadata": {},
   "source": [
    "# Initialize Parameters\n",
    "Initialize the model parameters using JAX's random number generation utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a109bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Parameters\n",
    "def initialize_parameters(layer_sizes, key):\n",
    "    params = []\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        w = jax.random.normal(subkey, (layer_sizes[i], layer_sizes[i + 1]))\n",
    "        b = jnp.zeros(layer_sizes[i + 1])\n",
    "        params.append((w, b))\n",
    "    return params\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "layer_sizes = [784, 128, 64, 10]  # Input layer, two hidden layers, output layer\n",
    "params = initialize_parameters(layer_sizes, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2a585f",
   "metadata": {},
   "source": [
    "# Define the Loss Function\n",
    "Implement the cross-entropy loss function for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34fdfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss Function\n",
    "def cross_entropy_loss(params, x, y):\n",
    "    logits = neural_network(params, x)\n",
    "    one_hot_y = jax.nn.one_hot(y, num_classes=10)\n",
    "    loss = -jnp.sum(one_hot_y * jax.nn.log_softmax(logits))\n",
    "    return loss / x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9198240",
   "metadata": {},
   "source": [
    "# Define the Training Loop\n",
    "Write a training loop that updates the model parameters using gradient descent or an optimizer from Optax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f59cc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Loop\n",
    "def train_model(params, train_data, optimizer, num_epochs, batch_size):\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    @jax.jit\n",
    "    def update(params, opt_state, x, y):\n",
    "        grads = jax.grad(cross_entropy_loss)(params, x, y)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in tfds.as_numpy(train_data.batch(batch_size)):\n",
    "            x_batch, y_batch = batch\n",
    "            params, opt_state = update(params, opt_state, x_batch, y_batch)\n",
    "        print(f\"Epoch {epoch + 1} completed.\")\n",
    "    return params\n",
    "\n",
    "optimizer = optax.adam(learning_rate=0.001)\n",
    "params = train_model(params, train_data, optimizer, num_epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1e8c66",
   "metadata": {},
   "source": [
    "# Evaluate the Model\n",
    "Evaluate the trained model on the test dataset and calculate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6a7f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "def evaluate_model(params, test_data):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in tfds.as_numpy(test_data.batch(32)):\n",
    "        x_batch, y_batch = batch\n",
    "        logits = neural_network(params, x_batch)\n",
    "        predictions = jnp.argmax(logits, axis=1)\n",
    "        correct += jnp.sum(predictions == y_batch)\n",
    "        total += y_batch.shape[0]\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "evaluate_model(params, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc2c521",
   "metadata": {},
   "source": [
    "# Install and Import Required Libraries\n",
    "Install JAX and other required libraries (e.g., numpy, matplotlib, tensorflow_datasets). Import necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cea62b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Libraries\n",
    "# Uncomment the following lines if running in an environment where JAX and other libraries are not installed.\n",
    "# !pip install jax jaxlib numpy matplotlib tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d940bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf310cfe",
   "metadata": {},
   "source": [
    "# Load and Preprocess the MNIST Dataset\n",
    "Use tensorflow_datasets to load the MNIST dataset. Normalize the images and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1ce3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST Dataset\n",
    "ds_train, ds_test = tfds.load('mnist', split=['train', 'test'], as_supervised=True)\n",
    "\n",
    "# Normalize and preprocess the dataset\n",
    "def preprocess(image, label):\n",
    "    image = jnp.array(image) / 255.0  # Normalize to [0, 1]\n",
    "    label = jnp.array(label)\n",
    "    return image, label\n",
    "\n",
    "ds_train = ds_train.map(preprocess)\n",
    "ds_test = ds_test.map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55110a6",
   "metadata": {},
   "source": [
    "# Define the Neural Network Model\n",
    "Use JAX to define a simple feedforward neural network with layers for MNIST classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c7f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Feedforward Neural Network\n",
    "def relu(x):\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = jnp.exp(x - jnp.max(x))\n",
    "    return exp_x / exp_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "def neural_network(params, x):\n",
    "    hidden = relu(jnp.dot(x, params['W1']) + params['b1'])\n",
    "    logits = jnp.dot(hidden, params['W2']) + params['b2']\n",
    "    return softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f15f6",
   "metadata": {},
   "source": [
    "# Initialize Parameters\n",
    "Randomly initialize the weights and biases of the neural network using JAX's random module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f0858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Parameters\n",
    "def initialize_params(key, input_dim, hidden_dim, output_dim):\n",
    "    key1, key2 = jax.random.split(key)\n",
    "    params = {\n",
    "        'W1': jax.random.normal(key1, (input_dim, hidden_dim)),\n",
    "        'b1': jnp.zeros(hidden_dim),\n",
    "        'W2': jax.random.normal(key2, (hidden_dim, output_dim)),\n",
    "        'b2': jnp.zeros(output_dim)\n",
    "    }\n",
    "    return params\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "params = initialize_params(key, input_dim=784, hidden_dim=128, output_dim=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bdd622",
   "metadata": {},
   "source": [
    "# Define the Loss Function\n",
    "Implement a cross-entropy loss function to measure the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3801d78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Entropy Loss Function\n",
    "def cross_entropy_loss(params, x, y):\n",
    "    predictions = neural_network(params, x)\n",
    "    one_hot_y = jax.nn.one_hot(y, num_classes=10)\n",
    "    return -jnp.mean(jnp.sum(one_hot_y * jnp.log(predictions), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da202954",
   "metadata": {},
   "source": [
    "# Define the Training Loop\n",
    "Write a training loop that uses JAX's grad function to compute gradients and update parameters using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446cc1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "@jax.jit\n",
    "def update_params(params, x, y, learning_rate):\n",
    "    grads = jax.grad(cross_entropy_loss)(params, x, y)\n",
    "    return jax.tree_util.tree_map(lambda p, g: p - learning_rate * g, params, grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcf6e26",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "Run the training loop for a specified number of epochs and track the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c9a769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in ds_train.batch(32):\n",
    "        images, labels = batch\n",
    "        images = images.reshape(-1, 784)  # Flatten images\n",
    "        params = update_params(params, images, labels, learning_rate)\n",
    "    print(f\"Epoch {epoch + 1} completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68fd02d",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import JAX, NumPy, and other necessary libraries such as matplotlib and tensorflow_datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7866de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb09340b",
   "metadata": {},
   "source": [
    "# Load and Preprocess the MNIST Dataset\n",
    "Load the MNIST dataset using tensorflow_datasets, normalize the images, and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4591bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Preprocess the MNIST Dataset\n",
    "def preprocess_data(data):\n",
    "    image = data['image'] / 255.0  # Normalize images to [0, 1]\n",
    "    label = data['label']\n",
    "    return image, label\n",
    "\n",
    "# Load dataset\n",
    "ds_train, ds_test = tfds.load('mnist', split=['train', 'test'], as_supervised=False)\n",
    "\n",
    "# Preprocess dataset\n",
    "train_data = [(preprocess_data(sample)) for sample in tfds.as_numpy(ds_train)]\n",
    "test_data = [(preprocess_data(sample)) for sample in tfds.as_numpy(ds_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a792dd",
   "metadata": {},
   "source": [
    "# Define the Neural Network Model\n",
    "Use JAX to define a simple feedforward neural network with one or more hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74551f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network Model\n",
    "def relu(x):\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "def neural_network(params, x):\n",
    "    hidden = relu(jnp.dot(x, params['W1']) + params['b1'])\n",
    "    logits = jnp.dot(hidden, params['W2']) + params['b2']\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83547a7",
   "metadata": {},
   "source": [
    "# Initialize Parameters\n",
    "Randomly initialize the weights and biases for the neural network using JAX's random module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9850b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Parameters\n",
    "def initialize_parameters(key, input_dim, hidden_dim, output_dim):\n",
    "    keys = jax.random.split(key, 2)\n",
    "    params = {\n",
    "        'W1': jax.random.normal(keys[0], (input_dim, hidden_dim)),\n",
    "        'b1': jnp.zeros(hidden_dim),\n",
    "        'W2': jax.random.normal(keys[1], (hidden_dim, output_dim)),\n",
    "        'b2': jnp.zeros(output_dim)\n",
    "    }\n",
    "    return params\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "params = initialize_parameters(key, input_dim=784, hidden_dim=128, output_dim=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b51066",
   "metadata": {},
   "source": [
    "# Define the Loss Function\n",
    "Implement the cross-entropy loss function to measure the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde8b75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Loss Function\n",
    "def cross_entropy_loss(params, x, y):\n",
    "    logits = neural_network(params, x)\n",
    "    one_hot_y = jax.nn.one_hot(y, num_classes=10)\n",
    "    loss = -jnp.sum(one_hot_y * jax.nn.log_softmax(logits))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55916c7b",
   "metadata": {},
   "source": [
    "# Define the Training Step\n",
    "Use JAX's grad function to compute gradients and update the model parameters using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0481de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Training Step\n",
    "@jax.jit\n",
    "def update_parameters(params, x, y, learning_rate):\n",
    "    grads = jax.grad(cross_entropy_loss)(params, x, y)\n",
    "    updated_params = jax.tree_util.tree_map(\n",
    "        lambda p, g: p - learning_rate * g, params, grads\n",
    "    )\n",
    "    return updated_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e6371a",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "Iteratively train the model over multiple epochs, updating the parameters and tracking the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fb24d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for x, y in train_data:\n",
    "        x = x.reshape(-1)  # Flatten the image\n",
    "        params = update_parameters(params, x, y, learning_rate)\n",
    "    print(f\"Epoch {epoch + 1} completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79549159",
   "metadata": {},
   "source": [
    "# Evaluate the Model\n",
    "Evaluate the trained model on the test dataset and calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb80aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "def evaluate_model(params, test_data):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for x, y in test_data:\n",
    "        x = x.reshape(-1)  # Flatten the image\n",
    "        logits = neural_network(params, x)\n",
    "        predicted_label = jnp.argmax(logits)\n",
    "        if predicted_label == y:\n",
    "            correct_predictions += 1\n",
    "        total_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "accuracy = evaluate_model(params, test_data)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80669f0e",
   "metadata": {},
   "source": [
    "# Install and Import Required Libraries\n",
    "Install JAX and other required libraries (e.g., numpy, matplotlib, tensorflow_datasets). Import all necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e347cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Libraries\n",
    "# Uncomment the following lines if running in an environment where JAX and other libraries are not installed.\n",
    "# !pip install jax jaxlib numpy matplotlib tensorflow-datasets\n",
    "\n",
    "# Import Required Libraries\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc1d38a",
   "metadata": {},
   "source": [
    "# Load and Preprocess the MNIST Dataset\n",
    "Use tensorflow_datasets to load the MNIST dataset. Normalize the images and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8070ab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST Dataset\n",
    "ds_builder = tfds.builder(\"mnist\")\n",
    "ds_builder.download_and_prepare()\n",
    "datasets = ds_builder.as_dataset(as_supervised=True)\n",
    "\n",
    "# Normalize and Split Data\n",
    "def preprocess(image, label):\n",
    "    image = jnp.array(image) / 255.0  # Normalize to [0, 1]\n",
    "    label = jnp.array(label)\n",
    "    return image, label\n",
    "\n",
    "train_dataset = datasets[\"train\"].map(preprocess)\n",
    "test_dataset = datasets[\"test\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a4b35",
   "metadata": {},
   "source": [
    "# Define the Neural Network Model\n",
    "Define a simple feedforward neural network using JAX's functional programming paradigm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcafef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Neural Network Model\n",
    "def neural_network(params, x):\n",
    "    for w, b in params[:-1]:\n",
    "        x = jnp.dot(x, w) + b\n",
    "        x = jax.nn.relu(x)\n",
    "    w, b = params[-1]\n",
    "    x = jnp.dot(x, w) + b\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b0cb77",
   "metadata": {},
   "source": [
    "# Initialize Parameters\n",
    "Randomly initialize the weights and biases of the neural network using JAX's random module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad81a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Parameters\n",
    "def initialize_params(layer_sizes, key):\n",
    "    params = []\n",
    "    for in_size, out_size in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        w = jax.random.normal(subkey, (in_size, out_size)) * jnp.sqrt(2.0 / in_size)\n",
    "        b = jnp.zeros(out_size)\n",
    "        params.append((w, b))\n",
    "    return params\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "layer_sizes = [784, 128, 64, 10]  # Input layer, two hidden layers, output layer\n",
    "params = initialize_params(layer_sizes, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a118a4f7",
   "metadata": {},
   "source": [
    "# Define the Loss Function\n",
    "Implement the cross-entropy loss function to measure the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4746a1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss Function\n",
    "def cross_entropy_loss(params, x, y):\n",
    "    logits = neural_network(params, x)\n",
    "    one_hot_y = jax.nn.one_hot(y, num_classes=10)\n",
    "    return -jnp.mean(jnp.sum(one_hot_y * jax.nn.log_softmax(logits), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4397de",
   "metadata": {},
   "source": [
    "# Define the Training Loop\n",
    "Write a training loop that computes gradients using JAX's grad function and updates the model parameters using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793cc8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Loop\n",
    "@jax.jit\n",
    "def update_params(params, x, y, lr):\n",
    "    grads = jax.grad(cross_entropy_loss)(params, x, y)\n",
    "    return [(w - lr * dw, b - lr * db) for (w, b), (dw, db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e7280c",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "Run the training loop for a specified number of epochs and monitor the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e254b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_dataset.batch(32):\n",
    "        images, labels = batch\n",
    "        images = images.reshape(-1, 784)  # Flatten images\n",
    "        params = update_params(params, images, labels, learning_rate)\n",
    "    print(f\"Epoch {epoch + 1} completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5201ebc3",
   "metadata": {},
   "source": [
    "# Evaluate the Model\n",
    "Evaluate the trained model on the test dataset and compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c456736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "def compute_accuracy(params, dataset):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in dataset.batch(32):\n",
    "        images, labels = batch\n",
    "        images = images.reshape(-1, 784)  # Flatten images\n",
    "        logits = neural_network(params, images)\n",
    "        predictions = jnp.argmax(logits, axis=-1)\n",
    "        correct += jnp.sum(predictions == labels)\n",
    "        total += len(labels)\n",
    "    return correct / total\n",
    "\n",
    "accuracy = compute_accuracy(params, test_dataset)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b668f2f8",
   "metadata": {},
   "source": [
    "# Install and Import Required Libraries\n",
    "Install JAX and other required libraries (e.g., numpy, matplotlib, tensorflow_datasets). Import all necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b668f2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jax in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.5.3)\n",
      "Requirement already satisfied: jaxlib in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.5.3)\n",
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.9.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.9.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: matplotlib in /home/codespace/.local/lib/python3.12/site-packages (3.10.1)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from jax) (0.5.1)\n",
      "Requirement already satisfied: numpy>=1.25 in /home/codespace/.local/lib/python3.12/site-packages (from jax) (2.2.4)\n",
      "Requirement already satisfied: opt_einsum in /usr/local/python/3.12.1/lib/python3.12/site-packages (from jax) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.11.1 in /home/codespace/.local/lib/python3.12/site-packages (from jax) (1.15.2)\n",
      "Collecting absl-py (from tensorflow-datasets)\n",
      "  Downloading absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: matplotlib in /home/codespace/.local/lib/python3.12/site-packages (3.10.1)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from jax) (0.5.1)\n",
      "Requirement already satisfied: numpy>=1.25 in /home/codespace/.local/lib/python3.12/site-packages (from jax) (2.2.4)\n",
      "Requirement already satisfied: opt_einsum in /usr/local/python/3.12.1/lib/python3.12/site-packages (from jax) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.11.1 in /home/codespace/.local/lib/python3.12/site-packages (from jax) (1.15.2)\n",
      "Collecting absl-py (from tensorflow-datasets)\n",
      "  Downloading absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting array_record>=0.5.0 (from tensorflow-datasets)\n",
      "  Downloading array_record-0.7.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (877 bytes)\n",
      "Collecting array_record>=0.5.0 (from tensorflow-datasets)\n",
      "  Downloading array_record-0.7.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (877 bytes)\n",
      "Collecting dm-tree (from tensorflow-datasets)\n",
      "  Downloading dm_tree-0.1.9-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting etils>=1.9.1 (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading etils-1.12.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting dm-tree (from tensorflow-datasets)\n",
      "  Downloading dm_tree-0.1.9-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting etils>=1.9.1 (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading etils-1.12.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting immutabledict (from tensorflow-datasets)\n",
      "  Downloading immutabledict-4.2.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting promise (from tensorflow-datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Collecting immutabledict (from tensorflow-datasets)\n",
      "  Downloading immutabledict-4.2.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting promise (from tensorflow-datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Installing build dependencies ... \u001b[?25l  Installing build dependencies ... \u001b[?25l-done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-done\n",
      "\u001b[?25done\n",
      "\u001b[?25hCollecting protobuf>=3.20 (from tensorflow-datasets)\n",
      "  Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting protobuf>=3.20 (from tensorflow-datasets)\n",
      "  Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: psutil in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow-datasets) (7.0.0)\n",
      "Requirement already satisfied: psutil in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow-datasets) (7.0.0)\n",
      "Collecting pyarrow (from tensorflow-datasets)\n",
      "  Downloading pyarrow-19.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow-datasets) (2.32.3)\n",
      "Collecting simple_parsing (from tensorflow-datasets)\n",
      "Collecting pyarrow (from tensorflow-datasets)\n",
      "  Downloading pyarrow-19.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow-datasets) (2.32.3)\n",
      "Collecting simple_parsing (from tensorflow-datasets)\n",
      "  Downloading simple_parsing-0.1.7-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow-datasets)\n",
      "  Downloading tensorflow_metadata-1.17.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting termcolor (from tensorflow-datasets)\n",
      "  Downloading simple_parsing-0.1.7-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow-datasets)\n",
      "  Downloading tensorflow_metadata-1.17.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting termcolor (from tensorflow-datasets)\n",
      "  Downloading termcolor-3.0.1-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting toml (from tensorflow-datasets)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading termcolor-3.0.1-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting toml (from tensorflow-datasets)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting tqdm (from tensorflow-datasets)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting tqdm (from tensorflow-datasets)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting wrapt (from tensorflow-datasets)\n",
      "  Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting wrapt (from tensorflow-datasets)\n",
      "  Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting einops (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (2024.6.1)\n",
      "Collecting einops (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (2024.6.1)\n",
      "Collecting importlib_resources (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing_extensions in /home/codespace/.local/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (4.12.2)\n",
      "Collecting importlib_resources (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing_extensions in /home/codespace/.local/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (4.12.2)\n",
      "Collecting zipp (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting zipp (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow-datasets) (2025.1.31)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from dm-tree->tensorflow-datasets) (25.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow-datasets) (2025.1.31)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from dm-tree->tensorflow-datasets) (25.3.0)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from simple_parsing->tensorflow-datasets)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.56.4 (from tensorflow-metadata->tensorflow-datasets)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from simple_parsing->tensorflow-datasets)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.56.4 (from tensorflow-metadata->tensorflow-datasets)\n",
      "  Downloading googleapis_common_protos-1.69.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting protobuf>=3.20 (from tensorflow-datasets)\n",
      "  Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "  Downloading googleapis_common_protos-1.69.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting protobuf>=3.20 (from tensorflow-datasets)\n",
      "  Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Downloading tensorflow_datasets-4.9.8-py3-none-any.whl (5.3 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading tensorflow_datasets-4.9.8-py3-none-any.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading array_record-0.7.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading array_record-0.7.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading etils-1.12.2-py3-none-any.whl (167 kB)\n",
      "Downloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "Downloading dm_tree-0.1.9-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (153 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading etils-1.12.2-py3-none-any.whl (167 kB)\n",
      "Downloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "Downloading dm_tree-0.1.9-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (153 kB)\n",
      "Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
      "Downloading immutabledict-4.2.1-py3-none-any.whl (4.7 kB)\n",
      "Downloading pyarrow-19.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
      "Downloading immutabledict-4.2.1-py3-none-any.whl (4.7 kB)\n",
      "Downloading pyarrow-19.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading simple_parsing-0.1.7-py3-none-any.whl (112 kB)\n",
      "Downloading tensorflow_metadata-1.17.0-py3-none-any.whl (29 kB)\n",
      "Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading termcolor-3.0.1-py3-none-any.whl (7.2 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading simple_parsing-0.1.7-py3-none-any.whl (112 kB)\n",
      "Downloading tensorflow_metadata-1.17.0-py3-none-any.whl (29 kB)\n",
      "Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading termcolor-3.0.1-py3-none-any.whl (7.2 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading googleapis_common_protos-1.69.2-py3-none-any.whl (293 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading googleapis_common_protos-1.69.2-py3-none-any.whl (293 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (pyproject.toml) ... \u001b[?25lBuilding wheels for collected packages: promise\n",
      "  Building wheel for promise (pyproject.toml) ... \u001b[?25l-done\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21581 sha256=31dc42bf4ae750bddcb3948f06868612fd880262bb9daafe393c83c3a12ea3db\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/e7/e6/28/864bdfee5339dbd6ddcb5a186286a8e217648ec198bdf0097d\n",
      "Successfully built promise\n",
      "\bdone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21581 sha256=31dc42bf4ae750bddcb3948f06868612fd880262bb9daafe393c83c3a12ea3db\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/e7/e6/28/864bdfee5339dbd6ddcb5a186286a8e217648ec198bdf0097d\n",
      "Successfully built promise\n",
      "Installing collected packages: zipp, wrapt, tqdm, toml, termcolor, pyarrow, protobuf, promise, importlib_resources, immutabledict, etils, einops, docstring-parser, absl-py, simple_parsing, googleapis-common-protos, dm-tree, tensorflow-metadata, array_record, tensorflow-datasets\n",
      "Installing collected packages: zipp, wrapt, tqdm, toml, termcolor, pyarrow, protobuf, promise, importlib_resources, immutabledict, etils, einops, docstring-parser, absl-py, simple_parsing, googleapis-common-protos, dm-tree, tensorflow-metadata, array_record, tensorflow-datasets\n",
      "Successfully installed absl-py-2.2.2 array_record-0.7.1 dm-tree-0.1.9 docstring-parser-0.16 einops-0.8.1 etils-1.12.2 googleapis-common-protos-1.69.2 immutabledict-4.2.1 importlib_resources-6.5.2 promise-2.3 protobuf-5.29.4 pyarrow-19.0.1 simple_parsing-0.1.7 tensorflow-datasets-4.9.8 tensorflow-metadata-1.17.0 termcolor-3.0.1 toml-0.10.2 tqdm-4.67.1 wrapt-1.17.2 zipp-3.21.0\n",
      "Successfully installed absl-py-2.2.2 array_record-0.7.1 dm-tree-0.1.9 docstring-parser-0.16 einops-0.8.1 etils-1.12.2 googleapis-common-protos-1.69.2 immutabledict-4.2.1 importlib_resources-6.5.2 promise-2.3 protobuf-5.29.4 pyarrow-19.0.1 simple_parsing-0.1.7 tensorflow-datasets-4.9.8 tensorflow-metadata-1.17.0 termcolor-3.0.1 toml-0.10.2 tqdm-4.67.1 wrapt-1.17.2 zipp-3.21.0\n"
     ]
    }
   ],
   "source": [
    "# Install Required Libraries\n",
    "!pip install jax jaxlib tensorflow-datasets matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c02bdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d6f12",
   "metadata": {},
   "source": [
    "# Load and Preprocess the MNIST Dataset\n",
    "Use tensorflow_datasets to load the MNIST dataset. Normalize the images and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3cfef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /home/codespace/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/2 [00:00<?, ? url/s]\n",
      "\n",
      "Dl Completed...:   0%|          | 0/2 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/3 [00:00<?, ? url/s]\u001b[A\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:00,  4.80 url/s]\n",
      "\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:00,  4.80 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  8.94 url/s]\n",
      "\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  8.94 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  8.68 url/s]\u001b[A\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  7.91 url/s]\n",
      "\u001b[A\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  7.19 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  7.07 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  7.19 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  7.07 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  6.60 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  9.53 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  6.60 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  9.53 url/s]\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\n",
      "Dl Size...:  50%|█████     | 5/10 [00:00<00:00, 15.35 MiB/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  9.13 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  9.13 url/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "***************************************************************\n",
      "Failed to import TensorFlow. Please note that TensorFlow is not installed by default when you install TFDS. This allows you to choose to install either `tf-nightly` or `tensorflow`. Please install the most recent version of TensorFlow, by following instructions at https://tensorflow.org/install.\n",
      "***************************************************************\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "***************************************************************\n",
      "Failed to import TensorFlow. Please note that TensorFlow is not installed by default when you install TFDS. This allows you to choose to install either `tf-nightly` or `tensorflow`. Please install the most recent version of TensorFlow, by following instructions at https://tensorflow.org/install.\n",
      "***************************************************************\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "***************************************************************\n",
      "Failed to import TensorFlow. Please note that TensorFlow is not installed by default when you install TFDS. This allows you to choose to install either `tf-nightly` or `tensorflow`. Please install the most recent version of TensorFlow, by following instructions at https://tensorflow.org/install.\n",
      "***************************************************************\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "***************************************************************\n",
      "Failed to import TensorFlow. Please note that TensorFlow is not installed by default when you install TFDS. This allows you to choose to install either `tf-nightly` or `tensorflow`. Please install the most recent version of TensorFlow, by following instructions at https://tensorflow.org/install.\n",
      "***************************************************************\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ExtractError",
     "evalue": "Error while extracting /home/codespace/tensorflow_datasets/downloads/mnist/cvdf-datasets_mnist_t10k-images-idx3-ubytejUIsewocHHkkWlvPB_6G4z7q_ueSuEWErsJ29aLbxOY.gz to /home/codespace/tensorflow_datasets/downloads/extracted/GZIP.cvdf-datasets_mnist_t10k-images-idx3-ubytejUIsewocHHkkWlvPB_6G4z7q_ueSuEWErsJ29aLbxOY.gz: No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/tensorflow_datasets/core/download/extractor.py:106\u001b[39m, in \u001b[36m_Extractor._extract\u001b[39m\u001b[34m(self, from_path, method, to_path)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrom_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/tensorflow_datasets/core/download/extractor.py:220\u001b[39m, in \u001b[36miter_gzip\u001b[39m\u001b[34m(arch_f)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34miter_gzip\u001b[39m(arch_f):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_open_or_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43march_f\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgzip_\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgzip\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGzipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/contextlib.py:137\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/tensorflow_datasets/core/download/extractor.py:166\u001b[39m, in \u001b[36m_open_or_pass\u001b[39m\u001b[34m(path_or_fobj)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_fobj, epath.PathLikeCls):\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mio\u001b[49m.gfile.GFile(path_or_fobj, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f_obj:\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m f_obj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/etils/epy/lazy_imports_utils.py:118\u001b[39m, in \u001b[36mLazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_module\u001b[49m, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/functools.py:995\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, instance, owner)\u001b[39m\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[32m--> \u001b[39m\u001b[32m995\u001b[39m     val = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    996\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/etils/epy/lazy_imports_utils.py:79\u001b[39m, in \u001b[36mLazyModule._module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     78\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m _LOCK_PER_MODULE[\u001b[38;5;28mself\u001b[39m.module_name]:\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1324\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mExtractError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load MNIST Dataset\u001b[39;00m\n\u001b[32m      2\u001b[39m ds_builder = tfds.builder(\u001b[33m\"\u001b[39m\u001b[33mmnist\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mds_builder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m datasets = ds_builder.as_dataset(as_supervised=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Normalize and split the dataset\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/tensorflow_datasets/core/logging/__init__.py:176\u001b[39m, in \u001b[36m_FunctionDecorator.__call__\u001b[39m\u001b[34m(self, function, instance, args, kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m metadata = \u001b[38;5;28mself\u001b[39m._start_call()\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    178\u001b[39m   metadata.mark_error()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/tensorflow_datasets/core/dataset_builder.py:763\u001b[39m, in \u001b[36mDatasetBuilder.download_and_prepare\u001b[39m\u001b[34m(self, download_dir, download_config, file_format, permissions)\u001b[39m\n\u001b[32m    761\u001b[39m   \u001b[38;5;28mself\u001b[39m.info.read_from_directory(\u001b[38;5;28mself\u001b[39m.data_dir)\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m763\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    766\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    768\u001b[39m   \u001b[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[32m    769\u001b[39m   \u001b[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[32m    770\u001b[39m   \u001b[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[32m    771\u001b[39m   \u001b[38;5;66;03m# when reading from package data.\u001b[39;00m\n\u001b[32m    772\u001b[39m   \u001b[38;5;28mself\u001b[39m.info.download_size = dl_manager.downloaded_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/tensorflow_datasets/core/dataset_builder.py:1808\u001b[39m, in \u001b[36mGeneratorBasedBuilder._download_and_prepare\u001b[39m\u001b[34m(self, dl_manager, download_config)\u001b[39m\n\u001b[32m   1805\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m download_config.max_examples_per_split == \u001b[32m0\u001b[39m:\n\u001b[32m   1806\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1808\u001b[39m split_infos = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_splits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1810\u001b[39m \u001b[38;5;66;03m# Update the info object with the splits.\u001b[39;00m\n\u001b[32m   1811\u001b[39m split_dict = splits_lib.SplitDict(split_infos)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/tensorflow_datasets/core/dataset_builder.py:1758\u001b[39m, in \u001b[36mGeneratorBasedBuilder._generate_splits\u001b[39m\u001b[34m(self, dl_manager, download_config)\u001b[39m\n\u001b[32m   1756\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1757\u001b[39m   optional_pipeline_kwargs = {}\n\u001b[32m-> \u001b[39m\u001b[32m1758\u001b[39m split_generators = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_split_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=unexpected-keyword-arg\u001b[39;49;00m\n\u001b[32m   1759\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptional_pipeline_kwargs\u001b[49m\n\u001b[32m   1760\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1761\u001b[39m \u001b[38;5;66;03m# TODO(tfds): Could be removed once all datasets are migrated.\u001b[39;00m\n\u001b[32m   1762\u001b[39m \u001b[38;5;66;03m# https://github.com/tensorflow/datasets/issues/2537\u001b[39;00m\n\u001b[32m   1763\u001b[39m \u001b[38;5;66;03m# Legacy mode (eventually convert list[SplitGeneratorLegacy] -> dict)\u001b[39;00m\n\u001b[32m   1764\u001b[39m split_generators = split_builder.normalize_legacy_split_generators(\n\u001b[32m   1765\u001b[39m     split_generators=split_generators,\n\u001b[32m   1766\u001b[39m     generator_fn=\u001b[38;5;28mself\u001b[39m._generate_examples,\n\u001b[32m   1767\u001b[39m     is_beam=\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, BeamBasedBuilder),\n\u001b[32m   1768\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/tensorflow_datasets/image_classification/mnist.py:119\u001b[39m, in \u001b[36mMNIST._split_generators\u001b[39m\u001b[34m(self, dl_manager)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# Download the full MNIST Database\u001b[39;00m\n\u001b[32m    113\u001b[39m filenames = {\n\u001b[32m    114\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtrain_data\u001b[39m\u001b[33m\"\u001b[39m: _MNIST_TRAIN_DATA_FILENAME,\n\u001b[32m    115\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtrain_labels\u001b[39m\u001b[33m\"\u001b[39m: _MNIST_TRAIN_LABELS_FILENAME,\n\u001b[32m    116\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtest_data\u001b[39m\u001b[33m\"\u001b[39m: _MNIST_TEST_DATA_FILENAME,\n\u001b[32m    117\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtest_labels\u001b[39m\u001b[33m\"\u001b[39m: _MNIST_TEST_LABELS_FILENAME,\n\u001b[32m    118\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m mnist_files = \u001b[43mdl_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_extract\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murllib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m.\u001b[49m\u001b[43murljoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mURL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# MNIST provides TRAIN and TEST splits, not a VALIDATION split, so we only\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# write the TRAIN and TEST splits to disk.\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    126\u001b[39m     tfds.core.SplitGenerator(\n\u001b[32m    127\u001b[39m         name=tfds.Split.TRAIN,\n\u001b[32m   (...)\u001b[39m\u001b[32m    141\u001b[39m     ),\n\u001b[32m    142\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/tensorflow_datasets/core/download/download_manager.py:754\u001b[39m, in \u001b[36mDownloadManager.download_and_extract\u001b[39m\u001b[34m(self, url_or_urls)\u001b[39m\n\u001b[32m    752\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._downloader.tqdm():\n\u001b[32m    753\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._extractor.tqdm():\n\u001b[32m--> \u001b[39m\u001b[32m754\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_promise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_extract\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/tensorflow_datasets/core/download/download_manager.py:782\u001b[39m, in \u001b[36m_map_promise\u001b[39m\u001b[34m(map_fn, all_inputs)\u001b[39m\n\u001b[32m    780\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[32m    781\u001b[39m all_promises = tree.map_structure(map_fn, all_inputs)  \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m res = \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_promises\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait promises\u001b[39;00m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/tree/__init__.py:428\u001b[39m, in \u001b[36mmap_structure\u001b[39m\u001b[34m(func, *structures, **kwargs)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[32m1\u001b[39m:]:\n\u001b[32m    426\u001b[39m   assert_same_structure(structures[\u001b[32m0\u001b[39m], other, check_types=check_types)\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[32m0\u001b[39m],\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m                     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(*\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/tensorflow_datasets/core/download/download_manager.py:782\u001b[39m, in \u001b[36m_map_promise.<locals>.<lambda>\u001b[39m\u001b[34m(p)\u001b[39m\n\u001b[32m    780\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[32m    781\u001b[39m all_promises = tree.map_structure(map_fn, all_inputs)  \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m res = tree.map_structure(\u001b[38;5;28;01mlambda\u001b[39;00m p: \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, all_promises)  \u001b[38;5;66;03m# Wait promises\u001b[39;00m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/promise/promise.py:512\u001b[39m, in \u001b[36mPromise.get\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    510\u001b[39m target = \u001b[38;5;28mself\u001b[39m._target()\n\u001b[32m    511\u001b[39m \u001b[38;5;28mself\u001b[39m._wait(timeout \u001b[38;5;129;01mor\u001b[39;00m DEFAULT_TIMEOUT)\n\u001b[32m--> \u001b[39m\u001b[32m512\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_target_settled_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_raise\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/promise/promise.py:516\u001b[39m, in \u001b[36mPromise._target_settled_value\u001b[39m\u001b[34m(self, _raise)\u001b[39m\n\u001b[32m    514\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_target_settled_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, _raise=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    515\u001b[39m     \u001b[38;5;66;03m# type: (bool) -> Any\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_settled_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_raise\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/promise/promise.py:226\u001b[39m, in \u001b[36mPromise._settled_value\u001b[39m\u001b[34m(self, _raise)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _raise:\n\u001b[32m    225\u001b[39m     raise_val = \u001b[38;5;28mself\u001b[39m._fulfillment_handler0\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraise_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_traceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fulfillment_handler0\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/six.py:724\u001b[39m, in \u001b[36mreraise\u001b[39m\u001b[34m(tp, value, tb)\u001b[39m\n\u001b[32m    722\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m value.__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[32m    723\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m value.with_traceback(tb)\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[32m    725\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    726\u001b[39m     value = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/promise/promise.py:844\u001b[39m, in \u001b[36m_process_future_result.<locals>.handle_future_result\u001b[39m\u001b[34m(future)\u001b[39m\n\u001b[32m    841\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhandle_future_result\u001b[39m(future):\n\u001b[32m    842\u001b[39m     \u001b[38;5;66;03m# type: (Any) -> None\u001b[39;00m\n\u001b[32m    843\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m844\u001b[39m         resolve(\u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    846\u001b[39m         tb = exc_info()[\u001b[32m2\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/concurrent/futures/thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/tensorflow_datasets/core/download/extractor.py:128\u001b[39m, in \u001b[36m_Extractor._extract\u001b[39m\u001b[34m(self, from_path, method, to_path)\u001b[39m\n\u001b[32m    122\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m os.name == \u001b[33m'\u001b[39m\u001b[33mnt\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m max_length_dst_path > \u001b[32m250\u001b[39m:\n\u001b[32m    123\u001b[39m     msg += (\n\u001b[32m    124\u001b[39m         \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOn windows, path lengths greater than 260 characters may result\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    125\u001b[39m         \u001b[33m'\u001b[39m\u001b[33m in an error. See the doc to remove the limitation: \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    126\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mhttps://docs.python.org/3/using/windows.html#removing-the-max-path-limitation\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    127\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ExtractError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# `tf.io.gfile.Rename(overwrite=True)` doesn't work for non empty\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# directories, so delete destination first, if it already exists.\u001b[39;00m\n\u001b[32m    132\u001b[39m to_path = epath.Path(to_path)\n",
      "\u001b[31mExtractError\u001b[39m: Error while extracting /home/codespace/tensorflow_datasets/downloads/mnist/cvdf-datasets_mnist_t10k-images-idx3-ubytejUIsewocHHkkWlvPB_6G4z7q_ueSuEWErsJ29aLbxOY.gz to /home/codespace/tensorflow_datasets/downloads/extracted/GZIP.cvdf-datasets_mnist_t10k-images-idx3-ubytejUIsewocHHkkWlvPB_6G4z7q_ueSuEWErsJ29aLbxOY.gz: No module named 'tensorflow'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "***************************************************************\n",
      "Failed to import TensorFlow. Please note that TensorFlow is not installed by default when you install TFDS. This allows you to choose to install either `tf-nightly` or `tensorflow`. Please install the most recent version of TensorFlow, by following instructions at https://tensorflow.org/install.\n",
      "***************************************************************\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "***************************************************************\n",
      "Failed to import TensorFlow. Please note that TensorFlow is not installed by default when you install TFDS. This allows you to choose to install either `tf-nightly` or `tensorflow`. Please install the most recent version of TensorFlow, by following instructions at https://tensorflow.org/install.\n",
      "***************************************************************\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "***************************************************************\n",
      "Failed to import TensorFlow. Please note that TensorFlow is not installed by default when you install TFDS. This allows you to choose to install either `tf-nightly` or `tensorflow`. Please install the most recent version of TensorFlow, by following instructions at https://tensorflow.org/install.\n",
      "***************************************************************\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "***************************************************************\n",
      "Failed to import TensorFlow. Please note that TensorFlow is not installed by default when you install TFDS. This allows you to choose to install either `tf-nightly` or `tensorflow`. Please install the most recent version of TensorFlow, by following instructions at https://tensorflow.org/install.\n",
      "***************************************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST Dataset\n",
    "ds_builder = tfds.builder(\"mnist\")\n",
    "ds_builder.download_and_prepare()\n",
    "datasets = ds_builder.as_dataset(as_supervised=True)\n",
    "\n",
    "# Normalize and split the dataset\n",
    "def preprocess(image, label):\n",
    "    image = jnp.array(image) / 255.0  # Normalize to [0, 1]\n",
    "    label = jnp.array(label)\n",
    "    return image, label\n",
    "\n",
    "train_ds = datasets[\"train\"].map(preprocess)\n",
    "test_ds = datasets[\"test\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ce63e8",
   "metadata": {},
   "source": [
    "# Define the Neural Network Model\n",
    "Use JAX to define a simple feedforward neural network model for MNIST classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afa73ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Neural Network Model\n",
    "def relu(x):\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = jnp.exp(x - jnp.max(x))\n",
    "    return exp_x / jnp.sum(exp_x)\n",
    "\n",
    "def neural_net(params, x):\n",
    "    hidden = relu(jnp.dot(x, params[\"W1\"]) + params[\"b1\"])\n",
    "    logits = jnp.dot(hidden, params[\"W2\"]) + params[\"b2\"]\n",
    "    return softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df4ade9",
   "metadata": {},
   "source": [
    "# Initialize Parameters\n",
    "Initialize the weights and biases of the neural network using JAX's random number generation utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed802e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Parameters\n",
    "def initialize_params(key, input_dim, hidden_dim, output_dim):\n",
    "    key1, key2 = jax.random.split(key)\n",
    "    params = {\n",
    "        \"W1\": jax.random.normal(key1, (input_dim, hidden_dim)),\n",
    "        \"b1\": jnp.zeros(hidden_dim),\n",
    "        \"W2\": jax.random.normal(key2, (hidden_dim, output_dim)),\n",
    "        \"b2\": jnp.zeros(output_dim),\n",
    "    }\n",
    "    return params\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "params = initialize_params(key, input_dim=784, hidden_dim=128, output_dim=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8759efaf",
   "metadata": {},
   "source": [
    "# Define the Loss Function\n",
    "Implement the cross-entropy loss function to measure the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8691fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss Function\n",
    "def cross_entropy_loss(params, x, y):\n",
    "    predictions = neural_net(params, x)\n",
    "    return -jnp.sum(jnp.log(predictions) * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b75d0f4",
   "metadata": {},
   "source": [
    "# Define the Training Loop\n",
    "Write a training loop that uses JAX's grad function to compute gradients and update the model parameters using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db08a9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Loop\n",
    "@jax.jit\n",
    "def update_params(params, x, y, learning_rate):\n",
    "    grads = jax.grad(cross_entropy_loss)(params, x, y)\n",
    "    updated_params = {k: v - learning_rate * grads[k] for k, v in params.items()}\n",
    "    return updated_params\n",
    "\n",
    "# Training Loop\n",
    "def train(params, train_ds, epochs, learning_rate):\n",
    "    for epoch in range(epochs):\n",
    "        for image, label in train_ds:\n",
    "            x = image.flatten()\n",
    "            y = jax.nn.one_hot(label, 10)\n",
    "            params = update_params(params, x, y, learning_rate)\n",
    "        print(f\"Epoch {epoch + 1} completed.\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73411c6",
   "metadata": {},
   "source": [
    "# Evaluate the Model\n",
    "Evaluate the trained model on the test dataset and calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b286d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "def evaluate(params, test_ds):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for image, label in test_ds:\n",
    "        x = image.flatten()\n",
    "        y_pred = jnp.argmax(neural_net(params, x))\n",
    "        if y_pred == label:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Example Usage\n",
    "trained_params = train(params, train_ds, epochs=5, learning_rate=0.01)\n",
    "evaluate(trained_params, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5a5fdb",
   "metadata": {},
   "source": [
    "# Install and Import Required Libraries\n",
    "Install JAX and other required libraries (e.g., numpy, matplotlib, tensorflow_datasets). Import necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5a5fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Libraries\n",
    "# Uncomment the following lines if running in an environment where JAX and other libraries are not installed.\n",
    "# !pip install jax jaxlib numpy matplotlib tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c799b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtfds\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2b9e06",
   "metadata": {},
   "source": [
    "# Load and Preprocess the MNIST Dataset\n",
    "Use tensorflow_datasets to load the MNIST dataset. Normalize the images and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86885a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST Dataset\n",
    "ds_builder = tfds.builder(\"mnist\")\n",
    "ds_builder.download_and_prepare()\n",
    "datasets = ds_builder.as_dataset(as_supervised=True)\n",
    "\n",
    "# Normalize and split the dataset\n",
    "def preprocess(image, label):\n",
    "    image = jnp.array(image) / 255.0  # Normalize to [0, 1]\n",
    "    label = jnp.array(label)\n",
    "    return image, label\n",
    "\n",
    "train_dataset = datasets[\"train\"].map(preprocess)\n",
    "test_dataset = datasets[\"test\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d2b168",
   "metadata": {},
   "source": [
    "# Define the Neural Network Model\n",
    "Define a simple feedforward neural network using JAX's functional programming paradigm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fef80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network Model\n",
    "def neural_net(params, x):\n",
    "    for w, b in params[:-1]:\n",
    "        x = jnp.dot(x, w) + b\n",
    "        x = jax.nn.relu(x)\n",
    "    final_w, final_b = params[-1]\n",
    "    x = jnp.dot(x, final_w) + final_b\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23245a71",
   "metadata": {},
   "source": [
    "# Initialize Model Parameters\n",
    "Initialize the weights and biases of the neural network using JAX's random number generation utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a55685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model Parameters\n",
    "def initialize_params(layer_sizes, key):\n",
    "    params = []\n",
    "    for in_size, out_size in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        w = jax.random.normal(subkey, (in_size, out_size)) * jnp.sqrt(2.0 / in_size)\n",
    "        b = jnp.zeros(out_size)\n",
    "        params.append((w, b))\n",
    "    return params\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "layer_sizes = [784, 128, 64, 10]  # Input layer, two hidden layers, output layer\n",
    "params = initialize_params(layer_sizes, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226666b1",
   "metadata": {},
   "source": [
    "# Define the Loss Function\n",
    "Implement a cross-entropy loss function to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a1ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Loss Function\n",
    "def cross_entropy_loss(params, x, y):\n",
    "    logits = neural_net(params, x)\n",
    "    one_hot_y = jax.nn.one_hot(y, num_classes=10)\n",
    "    return -jnp.mean(jnp.sum(one_hot_y * jax.nn.log_softmax(logits), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58689859",
   "metadata": {},
   "source": [
    "# Define the Training Loop\n",
    "Write a training loop that performs forward and backward passes, computes gradients, and updates model parameters using an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f75f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Training Loop\n",
    "@jax.jit\n",
    "def update(params, x, y, learning_rate):\n",
    "    grads = jax.grad(cross_entropy_loss)(params, x, y)\n",
    "    updated_params = [(w - learning_rate * dw, b - learning_rate * db) \n",
    "                      for (w, b), (dw, db) in zip(params, grads)]\n",
    "    return updated_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d00dd0",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "Run the training loop for a specified number of epochs and monitor the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a9fd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataset.batch(32):\n",
    "        images, labels = batch\n",
    "        images = images.reshape(-1, 784)  # Flatten images\n",
    "        params = update(params, images, labels, learning_rate)\n",
    "    print(f\"Epoch {epoch + 1} completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c67330",
   "metadata": {},
   "source": [
    "# Evaluate the Model\n",
    "Evaluate the trained model on the test dataset and compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e5d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "def accuracy(params, dataset):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in dataset.batch(32):\n",
    "        images, labels = batch\n",
    "        images = images.reshape(-1, 784)  # Flatten images\n",
    "        logits = neural_net(params, images)\n",
    "        predictions = jnp.argmax(logits, axis=-1)\n",
    "        correct += jnp.sum(predictions == labels)\n",
    "        total += len(labels)\n",
    "    return correct / total\n",
    "\n",
    "test_accuracy = accuracy(params, test_dataset)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
