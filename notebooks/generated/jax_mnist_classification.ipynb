{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abc8397a",
   "metadata": {},
   "source": [
    "# Install and Import Required Libraries\n",
    "Install JAX and import necessary libraries such as jax, jax.numpy, and optax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cc79f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install JAX\n",
    "# Note: Uncomment the following line if running in an environment where JAX is not installed.\n",
    "#!pip install jax jaxlib optax\n",
    "\n",
    "# Import Required Libraries\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, grad, jit\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468f1df3",
   "metadata": {},
   "source": [
    "# Load and Preprocess the MNIST Dataset\n",
    "Load the MNIST dataset, normalize the images, and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70c87eec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load MNIST Dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mnist\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[32m      5\u001b[39m (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Load MNIST Dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load data\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize images\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Flatten images\n",
    "train_images = train_images.reshape(-1, 28 * 28)\n",
    "test_images = test_images.reshape(-1, 28 * 28)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels = jax.nn.one_hot(train_labels, num_classes=10)\n",
    "test_labels = jax.nn.one_hot(test_labels, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd44b9d",
   "metadata": {},
   "source": [
    "# Define the Neural Network Model\n",
    "Define a simple feedforward neural network using JAX functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ec0932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Neural Network Model\n",
    "def neural_network(params, x):\n",
    "    for w, b in params[:-1]:\n",
    "        x = jnp.dot(x, w) + b\n",
    "        x = jax.nn.relu(x)\n",
    "    final_w, final_b = params[-1]\n",
    "    x = jnp.dot(x, final_w) + final_b\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21199871",
   "metadata": {},
   "source": [
    "# Initialize Parameters\n",
    "Initialize the model parameters using JAX's random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151b600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Parameters\n",
    "def initialize_parameters(layer_sizes, key):\n",
    "    params = []\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        key, subkey = random.split(key)\n",
    "        w = random.normal(subkey, (layer_sizes[i], layer_sizes[i + 1])) * jnp.sqrt(2.0 / layer_sizes[i])\n",
    "        b = jnp.zeros((layer_sizes[i + 1],))\n",
    "        params.append((w, b))\n",
    "    return params\n",
    "\n",
    "# Define layer sizes\n",
    "layer_sizes = [28 * 28, 128, 64, 10]\n",
    "key = random.PRNGKey(0)\n",
    "params = initialize_parameters(layer_sizes, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd19d8f",
   "metadata": {},
   "source": [
    "# Define the Loss Function\n",
    "Implement the cross-entropy loss function for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1870be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss Function\n",
    "def cross_entropy_loss(params, x, y):\n",
    "    logits = neural_network(params, x)\n",
    "    return -jnp.mean(jnp.sum(y * jax.nn.log_softmax(logits), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1f38b5",
   "metadata": {},
   "source": [
    "# Define the Training Loop\n",
    "Write a training loop to update model parameters using gradient descent or an optimizer from Optax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16744cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Loop\n",
    "@jit\n",
    "def update(params, x, y, opt_state, optimizer):\n",
    "    grads = grad(cross_entropy_loss)(params, x, y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optax.adam(learning_rate=0.001)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(train_images), batch_size):\n",
    "        batch_x = train_images[i:i + batch_size]\n",
    "        batch_y = train_labels[i:i + batch_size]\n",
    "        params, opt_state = update(params, batch_x, batch_y, opt_state, optimizer)\n",
    "    print(f\"Epoch {epoch + 1} completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1286fb3b",
   "metadata": {},
   "source": [
    "# Evaluate the Model on Test Data\n",
    "Evaluate the trained model on the test dataset and calculate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e11d959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "def accuracy(params, x, y):\n",
    "    predictions = jnp.argmax(neural_network(params, x), axis=1)\n",
    "    targets = jnp.argmax(y, axis=1)\n",
    "    return jnp.mean(predictions == targets)\n",
    "\n",
    "test_accuracy = accuracy(params, test_images, test_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd5139d",
   "metadata": {},
   "source": [
    "# Install and Import Required Libraries\n",
    "Install JAX and other required libraries (if not already installed). Import libraries such as jax, jax.numpy, and tensorflow_datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4180ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Libraries\n",
    "# Uncomment the following lines if the libraries are not already installed\n",
    "# !pip install jax jaxlib tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd096fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5573853",
   "metadata": {},
   "source": [
    "# Load and Preprocess the MNIST Dataset\n",
    "Use tensorflow_datasets to load the MNIST dataset. Normalize the images and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e71c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST Dataset\n",
    "ds_builder = tfds.builder(\"mnist\")\n",
    "ds_builder.download_and_prepare()\n",
    "datasets = ds_builder.as_dataset(as_supervised=True)\n",
    "\n",
    "# Normalize the images and split into training and testing sets\n",
    "def preprocess(image, label):\n",
    "    image = jnp.array(image) / 255.0  # Normalize to [0, 1]\n",
    "    label = jnp.array(label)\n",
    "    return image, label\n",
    "\n",
    "train_ds = datasets[\"train\"].map(preprocess)\n",
    "test_ds = datasets[\"test\"].map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7588b2c9",
   "metadata": {},
   "source": [
    "# Define the Neural Network Model\n",
    "Define a simple feedforward neural network using JAX functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a5a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network Model\n",
    "def neural_net(params, x):\n",
    "    for w, b in params[:-1]:\n",
    "        x = jnp.dot(x, w) + b\n",
    "        x = jax.nn.relu(x)\n",
    "    final_w, final_b = params[-1]\n",
    "    x = jnp.dot(x, final_w) + final_b\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc25d62",
   "metadata": {},
   "source": [
    "# Initialize Parameters\n",
    "Initialize the weights and biases of the neural network using JAX's random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c0f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Parameters\n",
    "def initialize_params(layer_sizes, key):\n",
    "    params = []\n",
    "    for n_in, n_out in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        w = jax.random.normal(subkey, (n_in, n_out)) * jnp.sqrt(2.0 / n_in)\n",
    "        b = jnp.zeros(n_out)\n",
    "        params.append((w, b))\n",
    "    return params\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "layer_sizes = [784, 128, 64, 10]  # Input layer, two hidden layers, output layer\n",
    "params = initialize_params(layer_sizes, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b775e0a4",
   "metadata": {},
   "source": [
    "# Define the Loss Function\n",
    "Implement a cross-entropy loss function to compute the difference between predictions and true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7863c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Loss Function\n",
    "def cross_entropy_loss(params, x, y):\n",
    "    logits = neural_net(params, x)\n",
    "    one_hot_y = jax.nn.one_hot(y, num_classes=10)\n",
    "    return -jnp.mean(jnp.sum(one_hot_y * jax.nn.log_softmax(logits), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda266da",
   "metadata": {},
   "source": [
    "# Define the Training Loop\n",
    "Write a training loop that updates the model parameters using gradient descent and JAX's grad function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e46023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Training Loop\n",
    "@jax.jit\n",
    "def update(params, x, y, lr):\n",
    "    grads = jax.grad(cross_entropy_loss)(params, x, y)\n",
    "    return [(w - lr * dw, b - lr * db) for (w, b), (dw, db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd51c75",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "Run the training loop for a specified number of epochs and monitor the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68482d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_ds.batch(32):\n",
    "        images, labels = batch\n",
    "        params = update(params, images, labels, learning_rate)\n",
    "    print(f\"Epoch {epoch + 1} completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9740a2a8",
   "metadata": {},
   "source": [
    "# Evaluate the Model\n",
    "Evaluate the trained model on the test dataset and compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de62f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "def accuracy(params, dataset):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in dataset.batch(32):\n",
    "        images, labels = batch\n",
    "        predictions = jnp.argmax(neural_net(params, images), axis=-1)\n",
    "        correct += jnp.sum(predictions == labels)\n",
    "        total += len(labels)\n",
    "    return correct / total\n",
    "\n",
    "test_accuracy = accuracy(params, test_ds)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
